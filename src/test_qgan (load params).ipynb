{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a74478",
   "metadata": {},
   "source": [
    "# Load parameters\n",
    "\n",
    "Add possibility of loading parameters of pretrained circuits to new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d261955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU device selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\f52ga\\miniforge3\\envs\\qgan\\Lib\\site-packages\\pennylane\\__init__.py:196: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.7.0 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from qgan.training import Training\n",
    "from config import CFG\n",
    "from tools.data.data_managers import save_model\n",
    "import os \n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f756515",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = copy.deepcopy(CFG)\n",
    "config.epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e02d7",
   "metadata": {},
   "source": [
    "## 1. Generator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949610f7",
   "metadata": {},
   "source": [
    "### 1.1. Same configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9649af",
   "metadata": {},
   "source": [
    "- Run training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e827d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== \n",
      "run_timestamp: 2025-11-13__10-04-48,\n",
      "----------------------------------------------\n",
      "load_timestamp: None,\n",
      "type_of_warm_start: none,\n",
      "warm_start_strength: 0.1,\n",
      "----------------------------------------------\n",
      "system_size: 3,\n",
      "extra_ancilla: False,\n",
      "ancilla_mode: pass,\n",
      "ancilla_project_norm: re-norm,\n",
      "ancilla_topology: bridge,\n",
      "ancilla_connect_to: None,\n",
      "do_ancilla_1q_gates: True,\n",
      "start_ancilla_gates_randomly: True,\n",
      "----------------------------------------------\n",
      "gen_layers: 3,\n",
      "gen_ansatz: ZZ_X_Z,\n",
      "----------------------------------------------\n",
      "target_hamiltonian: custom_h,\n",
      "custom_hamiltonian_terms: ['ZZZ'],\n",
      "custom_hamiltonian_strengths: [1.0],\n",
      "----------------------------------------------\n",
      "epochs: 2,\n",
      "iterations_epoch: 300,\n",
      "log_every_x_iter: 10,\n",
      "save_fid_and_loss_every_x_iter: 1,\n",
      "max_fidelity: 0.99,\n",
      "steps_gen: 1,\n",
      "steps_dis: 1,\n",
      "----------------------------------------------\n",
      "l_rate: 0.01,\n",
      "momentum_coeff: 0.9,\n",
      "================================================== \n",
      "\n",
      "\n",
      "Epoch:    1 | Iter:    1 | Fidelity: 0.074690 | Loss: -3.880449\n",
      "\n",
      "Epoch:    1 | Iter:   11 | Fidelity: 0.078580 | Loss: -3.493678\n",
      "\n",
      "Epoch:    1 | Iter:   21 | Fidelity: 0.084538 | Loss: -3.335150\n",
      "\n",
      "Epoch:    1 | Iter:   31 | Fidelity: 0.090600 | Loss: -3.100988\n",
      "\n",
      "Epoch:    1 | Iter:   41 | Fidelity: 0.094221 | Loss: -2.952538\n",
      "\n",
      "Epoch:    1 | Iter:   51 | Fidelity: 0.091009 | Loss: -2.834683\n",
      "\n",
      "Epoch:    1 | Iter:   61 | Fidelity: 0.079598 | Loss: -2.700577\n",
      "\n",
      "Epoch:    1 | Iter:   71 | Fidelity: 0.064753 | Loss: -2.536975\n",
      "\n",
      "Epoch:    1 | Iter:   81 | Fidelity: 0.055068 | Loss: -2.530681\n",
      "\n",
      "Epoch:    1 | Iter:   91 | Fidelity: 0.054951 | Loss: -2.518874\n",
      "\n",
      "Epoch:    1 | Iter:  101 | Fidelity: 0.064237 | Loss: -2.512301\n",
      "\n",
      "Epoch:    1 | Iter:  111 | Fidelity: 0.080540 | Loss: -2.493775\n",
      "\n",
      "Epoch:    1 | Iter:  121 | Fidelity: 0.101166 | Loss: -2.423360\n",
      "\n",
      "Epoch:    1 | Iter:  131 | Fidelity: 0.124102 | Loss: -2.098517\n",
      "\n",
      "Epoch:    1 | Iter:  141 | Fidelity: 0.149881 | Loss: -0.355038\n",
      "\n",
      "Epoch:    1 | Iter:  151 | Fidelity: 0.184356 | Loss: 0.159065\n",
      "\n",
      "Epoch:    1 | Iter:  161 | Fidelity: 0.224380 | Loss: 0.282079\n",
      "\n",
      "Epoch:    1 | Iter:  171 | Fidelity: 0.264898 | Loss: 0.308475\n",
      "\n",
      "Epoch:    1 | Iter:  181 | Fidelity: 0.300922 | Loss: 0.322721\n",
      "\n",
      "Epoch:    1 | Iter:  191 | Fidelity: 0.329721 | Loss: 0.321185\n",
      "\n",
      "Epoch:    1 | Iter:  201 | Fidelity: 0.351116 | Loss: 0.312091\n",
      "\n",
      "Epoch:    1 | Iter:  211 | Fidelity: 0.366643 | Loss: 0.305202\n",
      "\n",
      "Epoch:    1 | Iter:  221 | Fidelity: 0.378272 | Loss: 0.300050\n",
      "\n",
      "Epoch:    1 | Iter:  231 | Fidelity: 0.387705 | Loss: 0.295672\n",
      "\n",
      "Epoch:    1 | Iter:  241 | Fidelity: 0.396087 | Loss: 0.291643\n",
      "\n",
      "Epoch:    1 | Iter:  251 | Fidelity: 0.404096 | Loss: 0.287767\n",
      "\n",
      "Epoch:    1 | Iter:  261 | Fidelity: 0.412057 | Loss: 0.283874\n",
      "\n",
      "Epoch:    1 | Iter:  271 | Fidelity: 0.420049 | Loss: 0.279935\n",
      "\n",
      "Epoch:    1 | Iter:  281 | Fidelity: 0.427969 | Loss: 0.276016\n",
      "\n",
      "Epoch:    1 | Iter:  291 | Fidelity: 0.435590 | Loss: 0.272241\n",
      "\n",
      "Epoch:    2 | Iter:    1 | Fidelity: 0.442632 | Loss: 0.268754\n",
      "\n",
      "Epoch:    2 | Iter:   11 | Fidelity: 0.448843 | Loss: 0.265678\n",
      "\n",
      "Epoch:    2 | Iter:   21 | Fidelity: 0.454064 | Loss: 0.263093\n",
      "\n",
      "Epoch:    2 | Iter:   31 | Fidelity: 0.458262 | Loss: 0.261024\n",
      "\n",
      "Epoch:    2 | Iter:   41 | Fidelity: 0.461515 | Loss: 0.259425\n",
      "\n",
      "Epoch:    2 | Iter:   51 | Fidelity: 0.463972 | Loss: 0.258224\n",
      "\n",
      "Epoch:    2 | Iter:   61 | Fidelity: 0.465807 | Loss: 0.257326\n",
      "\n",
      "Epoch:    2 | Iter:   71 | Fidelity: 0.467181 | Loss: 0.256663\n",
      "\n",
      "Epoch:    2 | Iter:   81 | Fidelity: 0.468227 | Loss: 0.256167\n",
      "\n",
      "Epoch:    2 | Iter:   91 | Fidelity: 0.469042 | Loss: 0.255780\n",
      "\n",
      "Epoch:    2 | Iter:  101 | Fidelity: 0.469695 | Loss: 0.255474\n",
      "\n",
      "Epoch:    2 | Iter:  111 | Fidelity: 0.470234 | Loss: 0.255228\n",
      "\n",
      "Epoch:    2 | Iter:  121 | Fidelity: 0.470689 | Loss: 0.255017\n",
      "\n",
      "Epoch:    2 | Iter:  131 | Fidelity: 0.471080 | Loss: 0.254840\n",
      "\n",
      "Epoch:    2 | Iter:  141 | Fidelity: 0.471422 | Loss: 0.254686\n",
      "\n",
      "Epoch:    2 | Iter:  151 | Fidelity: 0.471725 | Loss: 0.254554\n",
      "\n",
      "Epoch:    2 | Iter:  161 | Fidelity: 0.471995 | Loss: 0.254432\n",
      "\n",
      "Epoch:    2 | Iter:  171 | Fidelity: 0.472237 | Loss: 0.254325\n",
      "\n",
      "Epoch:    2 | Iter:  181 | Fidelity: 0.472456 | Loss: 0.254229\n",
      "\n",
      "Epoch:    2 | Iter:  191 | Fidelity: 0.472655 | Loss: 0.254138\n",
      "\n",
      "Epoch:    2 | Iter:  201 | Fidelity: 0.472837 | Loss: 0.254061\n",
      "\n",
      "Epoch:    2 | Iter:  211 | Fidelity: 0.473003 | Loss: 0.253984\n",
      "\n",
      "Epoch:    2 | Iter:  221 | Fidelity: 0.473156 | Loss: 0.253922\n",
      "\n",
      "Epoch:    2 | Iter:  231 | Fidelity: 0.473297 | Loss: 0.253859\n",
      "\n",
      "Epoch:    2 | Iter:  241 | Fidelity: 0.473427 | Loss: 0.253803\n",
      "\n",
      "Epoch:    2 | Iter:  251 | Fidelity: 0.473548 | Loss: 0.253748\n",
      "\n",
      "Epoch:    2 | Iter:  261 | Fidelity: 0.473661 | Loss: 0.253700\n",
      "\n",
      "Epoch:    2 | Iter:  271 | Fidelity: 0.473767 | Loss: 0.253654\n",
      "\n",
      "Epoch:    2 | Iter:  281 | Fidelity: 0.473865 | Loss: 0.253608\n",
      "\n",
      "Epoch:    2 | Iter:  291 | Fidelity: 0.473957 | Loss: 0.253569\n",
      "\n",
      "Training finished after 2 epochs.\n",
      "\n",
      "Run took: 88.48948109999765 time.\n",
      "Generator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-gen(hs).pkl\n",
      "Discriminator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-dis(hs).pkl\n"
     ]
    }
   ],
   "source": [
    "training_instance = Training(config=config)\n",
    "res = training_instance.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67950bca",
   "metadata": {},
   "source": [
    "- Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96a575a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator parameters saved to ./models/training_instance.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.9405, -0.7583,  0.2513, -0.7750,  0.5361, -0.7160,  0.0131, -2.4878,\n",
       "        -1.0326, -1.2312, -0.2089,  0.3864, -0.5155,  0.3836,  0.0685, -0.3802,\n",
       "         0.6317,  0.0951,  0.4081, -1.1650,  0.9608, -1.1496, -1.6506,  1.2068],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./models/training_instance.pkl\"\n",
    "training_instance.gen.save_model_params(path)\n",
    "training_instance.gen.ansatz.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87820cf0",
   "metadata": {},
   "source": [
    "- Load parameters in new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "126a4354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator parameters loaded successfully from ./models/training_instance.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.9405, -0.7583,  0.2513, -0.7750,  0.5361, -0.7160,  0.0131, -2.4878,\n",
       "        -1.0326, -1.2312, -0.2089,  0.3864, -0.5155,  0.3836,  0.0685, -0.3802,\n",
       "         0.6317,  0.0951,  0.4081, -1.1650,  0.9608, -1.1496, -1.6506,  1.2068],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./models/training_instance.pkl\"\n",
    "training_instance2 = Training(config=config)\n",
    "training_instance2.gen.load_model_params(path)\n",
    "training_instance2.gen.ansatz.theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d5133",
   "metadata": {},
   "source": [
    "- Retrain training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865c3c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== \n",
      "run_timestamp: 2025-11-13__10-04-48,\n",
      "----------------------------------------------\n",
      "load_timestamp: None,\n",
      "type_of_warm_start: none,\n",
      "warm_start_strength: 0.1,\n",
      "----------------------------------------------\n",
      "system_size: 3,\n",
      "extra_ancilla: False,\n",
      "ancilla_mode: pass,\n",
      "ancilla_project_norm: re-norm,\n",
      "ancilla_topology: bridge,\n",
      "ancilla_connect_to: None,\n",
      "do_ancilla_1q_gates: True,\n",
      "start_ancilla_gates_randomly: True,\n",
      "----------------------------------------------\n",
      "gen_layers: 3,\n",
      "gen_ansatz: ZZ_X_Z,\n",
      "----------------------------------------------\n",
      "target_hamiltonian: custom_h,\n",
      "custom_hamiltonian_terms: ['ZZZ'],\n",
      "custom_hamiltonian_strengths: [1.0],\n",
      "----------------------------------------------\n",
      "epochs: 2,\n",
      "iterations_epoch: 300,\n",
      "log_every_x_iter: 10,\n",
      "save_fid_and_loss_every_x_iter: 1,\n",
      "max_fidelity: 0.99,\n",
      "steps_gen: 1,\n",
      "steps_dis: 1,\n",
      "----------------------------------------------\n",
      "l_rate: 0.01,\n",
      "momentum_coeff: 0.9,\n",
      "================================================== \n",
      "\n",
      "\n",
      "Epoch:    1 | Iter:    1 | Fidelity: 0.474017 | Loss: -3.804128\n",
      "\n",
      "Epoch:    1 | Iter:   11 | Fidelity: 0.462017 | Loss: -3.503835\n",
      "\n",
      "Epoch:    1 | Iter:   21 | Fidelity: 0.434485 | Loss: -3.276094\n",
      "\n",
      "Epoch:    1 | Iter:   31 | Fidelity: 0.423709 | Loss: -3.125587\n",
      "\n",
      "Epoch:    1 | Iter:   41 | Fidelity: 0.429954 | Loss: -2.997758\n",
      "\n",
      "Epoch:    1 | Iter:   51 | Fidelity: 0.440760 | Loss: -2.871246\n",
      "\n",
      "Epoch:    1 | Iter:   61 | Fidelity: 0.440643 | Loss: -2.723423\n",
      "\n",
      "Epoch:    1 | Iter:   71 | Fidelity: 0.413955 | Loss: -2.601940\n",
      "\n",
      "Epoch:    1 | Iter:   81 | Fidelity: 0.362156 | Loss: -2.440460\n",
      "\n",
      "Epoch:    1 | Iter:   91 | Fidelity: 0.309697 | Loss: -1.941089\n",
      "\n",
      "Epoch:    1 | Iter:  101 | Fidelity: 0.281906 | Loss: -0.435117\n",
      "\n",
      "Epoch:    1 | Iter:  111 | Fidelity: 0.298624 | Loss: 0.016421\n",
      "\n",
      "Epoch:    1 | Iter:  121 | Fidelity: 0.335476 | Loss: 0.265882\n",
      "\n",
      "Epoch:    1 | Iter:  131 | Fidelity: 0.375914 | Loss: 0.272520\n",
      "\n",
      "Epoch:    1 | Iter:  141 | Fidelity: 0.409591 | Loss: 0.271375\n",
      "\n",
      "Epoch:    1 | Iter:  151 | Fidelity: 0.432985 | Loss: 0.268123\n",
      "\n",
      "Epoch:    1 | Iter:  161 | Fidelity: 0.447602 | Loss: 0.264351\n",
      "\n",
      "Epoch:    1 | Iter:  171 | Fidelity: 0.456400 | Loss: 0.261208\n",
      "\n",
      "Epoch:    1 | Iter:  181 | Fidelity: 0.461749 | Loss: 0.259192\n",
      "\n",
      "Epoch:    1 | Iter:  191 | Fidelity: 0.465215 | Loss: 0.257960\n",
      "\n",
      "Epoch:    1 | Iter:  201 | Fidelity: 0.467568 | Loss: 0.257135\n",
      "\n",
      "Epoch:    1 | Iter:  211 | Fidelity: 0.469227 | Loss: 0.256577\n",
      "\n",
      "Epoch:    1 | Iter:  221 | Fidelity: 0.470416 | Loss: 0.256206\n",
      "\n",
      "Epoch:    1 | Iter:  231 | Fidelity: 0.471280 | Loss: 0.255953\n",
      "\n",
      "Epoch:    1 | Iter:  241 | Fidelity: 0.471919 | Loss: 0.255786\n",
      "\n",
      "Epoch:    1 | Iter:  251 | Fidelity: 0.472402 | Loss: 0.255677\n",
      "\n",
      "Epoch:    1 | Iter:  261 | Fidelity: 0.472779 | Loss: 0.255601\n",
      "\n",
      "Epoch:    1 | Iter:  271 | Fidelity: 0.473082 | Loss: 0.255551\n",
      "\n",
      "Epoch:    1 | Iter:  281 | Fidelity: 0.473332 | Loss: 0.255513\n",
      "\n",
      "Epoch:    1 | Iter:  291 | Fidelity: 0.473545 | Loss: 0.255483\n",
      "\n",
      "Epoch:    2 | Iter:    1 | Fidelity: 0.473729 | Loss: 0.255457\n",
      "\n",
      "Epoch:    2 | Iter:   11 | Fidelity: 0.473892 | Loss: 0.255431\n",
      "\n",
      "Epoch:    2 | Iter:   21 | Fidelity: 0.474037 | Loss: 0.255414\n",
      "\n",
      "Epoch:    2 | Iter:   31 | Fidelity: 0.474167 | Loss: 0.255396\n",
      "\n",
      "Epoch:    2 | Iter:   41 | Fidelity: 0.474285 | Loss: 0.255383\n",
      "\n",
      "Epoch:    2 | Iter:   51 | Fidelity: 0.474392 | Loss: 0.255365\n",
      "\n",
      "Epoch:    2 | Iter:   61 | Fidelity: 0.474490 | Loss: 0.255354\n",
      "\n",
      "Epoch:    2 | Iter:   71 | Fidelity: 0.474581 | Loss: 0.255342\n",
      "\n",
      "Epoch:    2 | Iter:   81 | Fidelity: 0.474664 | Loss: 0.255333\n",
      "\n",
      "Epoch:    2 | Iter:   91 | Fidelity: 0.474741 | Loss: 0.255322\n",
      "\n",
      "Epoch:    2 | Iter:  101 | Fidelity: 0.474813 | Loss: 0.255311\n",
      "\n",
      "Epoch:    2 | Iter:  111 | Fidelity: 0.474880 | Loss: 0.255303\n",
      "\n",
      "Epoch:    2 | Iter:  121 | Fidelity: 0.474943 | Loss: 0.255289\n",
      "\n",
      "Epoch:    2 | Iter:  131 | Fidelity: 0.475001 | Loss: 0.255289\n",
      "\n",
      "Epoch:    2 | Iter:  141 | Fidelity: 0.475057 | Loss: 0.255278\n",
      "\n",
      "Epoch:    2 | Iter:  151 | Fidelity: 0.475109 | Loss: 0.255273\n",
      "\n",
      "Epoch:    2 | Iter:  161 | Fidelity: 0.475158 | Loss: 0.255266\n",
      "\n",
      "Epoch:    2 | Iter:  171 | Fidelity: 0.475205 | Loss: 0.255260\n",
      "\n",
      "Epoch:    2 | Iter:  181 | Fidelity: 0.475249 | Loss: 0.255253\n",
      "\n",
      "Epoch:    2 | Iter:  191 | Fidelity: 0.475291 | Loss: 0.255245\n",
      "\n",
      "Epoch:    2 | Iter:  201 | Fidelity: 0.475331 | Loss: 0.255239\n",
      "\n",
      "Epoch:    2 | Iter:  211 | Fidelity: 0.475370 | Loss: 0.255227\n",
      "\n",
      "Epoch:    2 | Iter:  221 | Fidelity: 0.475407 | Loss: 0.255225\n",
      "\n",
      "Epoch:    2 | Iter:  231 | Fidelity: 0.475442 | Loss: 0.255219\n",
      "\n",
      "Epoch:    2 | Iter:  241 | Fidelity: 0.475475 | Loss: 0.255216\n",
      "\n",
      "Epoch:    2 | Iter:  251 | Fidelity: 0.475508 | Loss: 0.255211\n",
      "\n",
      "Epoch:    2 | Iter:  261 | Fidelity: 0.475539 | Loss: 0.255205\n",
      "\n",
      "Epoch:    2 | Iter:  271 | Fidelity: 0.475569 | Loss: 0.255202\n",
      "\n",
      "Epoch:    2 | Iter:  281 | Fidelity: 0.475598 | Loss: 0.255194\n",
      "\n",
      "Epoch:    2 | Iter:  291 | Fidelity: 0.475626 | Loss: 0.255190\n",
      "\n",
      "Training finished after 2 epochs.\n",
      "\n",
      "Run took: 39.22520980006084 time.\n",
      "Generator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-gen(hs).pkl\n",
      "Discriminator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-dis(hs).pkl\n"
     ]
    }
   ],
   "source": [
    "res2 = training_instance2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7876c",
   "metadata": {},
   "source": [
    "## 2. Discriminator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e78207",
   "metadata": {},
   "source": [
    "### 2.1. Same configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca3edc",
   "metadata": {},
   "source": [
    "We load both generator and discriminator parameters since the generator is before in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496804ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = copy.deepcopy(CFG)\n",
    "config.epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a62c5",
   "metadata": {},
   "source": [
    "- Run training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad1ece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== \n",
      "run_timestamp: 2025-11-13__10-04-48,\n",
      "----------------------------------------------\n",
      "load_timestamp: None,\n",
      "type_of_warm_start: none,\n",
      "warm_start_strength: 0.1,\n",
      "----------------------------------------------\n",
      "system_size: 3,\n",
      "extra_ancilla: False,\n",
      "ancilla_mode: pass,\n",
      "ancilla_project_norm: re-norm,\n",
      "ancilla_topology: bridge,\n",
      "ancilla_connect_to: None,\n",
      "do_ancilla_1q_gates: True,\n",
      "start_ancilla_gates_randomly: True,\n",
      "----------------------------------------------\n",
      "gen_layers: 3,\n",
      "gen_ansatz: ZZ_X_Z,\n",
      "----------------------------------------------\n",
      "target_hamiltonian: custom_h,\n",
      "custom_hamiltonian_terms: ['ZZZ'],\n",
      "custom_hamiltonian_strengths: [1.0],\n",
      "----------------------------------------------\n",
      "epochs: 2,\n",
      "iterations_epoch: 300,\n",
      "log_every_x_iter: 10,\n",
      "save_fid_and_loss_every_x_iter: 1,\n",
      "max_fidelity: 0.99,\n",
      "steps_gen: 1,\n",
      "steps_dis: 1,\n",
      "----------------------------------------------\n",
      "l_rate: 0.01,\n",
      "momentum_coeff: 0.9,\n",
      "================================================== \n",
      "\n",
      "\n",
      "Epoch:    1 | Iter:    1 | Fidelity: 0.022805 | Loss: -3.509424\n",
      "\n",
      "Epoch:    1 | Iter:   11 | Fidelity: 0.022020 | Loss: -3.480903\n",
      "\n",
      "Epoch:    1 | Iter:   21 | Fidelity: 0.020941 | Loss: -3.426569\n",
      "\n",
      "Epoch:    1 | Iter:   31 | Fidelity: 0.020147 | Loss: -3.319358\n",
      "\n",
      "Epoch:    1 | Iter:   41 | Fidelity: 0.019756 | Loss: -3.149135\n",
      "\n",
      "Epoch:    1 | Iter:   51 | Fidelity: 0.019696 | Loss: -2.946050\n",
      "\n",
      "Epoch:    1 | Iter:   61 | Fidelity: 0.019835 | Loss: -2.471617\n",
      "\n",
      "Epoch:    1 | Iter:   71 | Fidelity: 0.020176 | Loss: -1.851634\n",
      "\n",
      "Epoch:    1 | Iter:   81 | Fidelity: 0.021025 | Loss: -1.532607\n",
      "\n",
      "Epoch:    1 | Iter:   91 | Fidelity: 0.022373 | Loss: -0.124162\n",
      "\n",
      "Epoch:    1 | Iter:  101 | Fidelity: 0.024482 | Loss: -0.208974\n",
      "\n",
      "Epoch:    1 | Iter:  111 | Fidelity: 0.027001 | Loss: -0.141768\n",
      "\n",
      "Epoch:    1 | Iter:  121 | Fidelity: 0.029895 | Loss: -0.079664\n",
      "\n",
      "Epoch:    1 | Iter:  131 | Fidelity: 0.033224 | Loss: -0.074652\n",
      "\n",
      "Epoch:    1 | Iter:  141 | Fidelity: 0.037012 | Loss: -0.074670\n",
      "\n",
      "Epoch:    1 | Iter:  151 | Fidelity: 0.041358 | Loss: -0.073823\n",
      "\n",
      "Epoch:    1 | Iter:  161 | Fidelity: 0.046361 | Loss: -0.075282\n",
      "\n",
      "Epoch:    1 | Iter:  171 | Fidelity: 0.052144 | Loss: -0.077499\n",
      "\n",
      "Epoch:    1 | Iter:  181 | Fidelity: 0.058864 | Loss: -0.080035\n",
      "\n",
      "Epoch:    1 | Iter:  191 | Fidelity: 0.066702 | Loss: -0.083109\n",
      "\n",
      "Epoch:    1 | Iter:  201 | Fidelity: 0.075880 | Loss: -0.086792\n",
      "\n",
      "Epoch:    1 | Iter:  211 | Fidelity: 0.086660 | Loss: -0.091144\n",
      "\n",
      "Epoch:    1 | Iter:  221 | Fidelity: 0.099351 | Loss: -0.096283\n",
      "\n",
      "Epoch:    1 | Iter:  231 | Fidelity: 0.114310 | Loss: -0.102345\n",
      "\n",
      "Epoch:    1 | Iter:  241 | Fidelity: 0.131943 | Loss: -0.109484\n",
      "\n",
      "Epoch:    1 | Iter:  251 | Fidelity: 0.152694 | Loss: -0.117867\n",
      "\n",
      "Epoch:    1 | Iter:  261 | Fidelity: 0.177026 | Loss: -0.127666\n",
      "\n",
      "Epoch:    1 | Iter:  271 | Fidelity: 0.205389 | Loss: -0.139047\n",
      "\n",
      "Epoch:    1 | Iter:  281 | Fidelity: 0.238162 | Loss: -0.152137\n",
      "\n",
      "Epoch:    1 | Iter:  291 | Fidelity: 0.275581 | Loss: -0.167013\n",
      "\n",
      "Epoch:    2 | Iter:    1 | Fidelity: 0.317618 | Loss: -0.183664\n",
      "\n",
      "Epoch:    2 | Iter:   11 | Fidelity: 0.363826 | Loss: -0.201901\n",
      "\n",
      "Epoch:    2 | Iter:   21 | Fidelity: 0.413143 | Loss: -0.221327\n",
      "\n",
      "Epoch:    2 | Iter:   31 | Fidelity: 0.463729 | Loss: -0.241248\n",
      "\n",
      "Epoch:    2 | Iter:   41 | Fidelity: 0.513000 | Loss: -0.260666\n",
      "\n",
      "Epoch:    2 | Iter:   51 | Fidelity: 0.558062 | Loss: -0.278461\n",
      "\n",
      "Epoch:    2 | Iter:   61 | Fidelity: 0.596480 | Loss: -0.293663\n",
      "\n",
      "Epoch:    2 | Iter:   71 | Fidelity: 0.627021 | Loss: -0.305771\n",
      "\n",
      "Epoch:    2 | Iter:   81 | Fidelity: 0.649866 | Loss: -0.314845\n",
      "\n",
      "Epoch:    2 | Iter:   91 | Fidelity: 0.666213 | Loss: -0.321341\n",
      "\n",
      "Epoch:    2 | Iter:  101 | Fidelity: 0.677634 | Loss: -0.325883\n",
      "\n",
      "Epoch:    2 | Iter:  111 | Fidelity: 0.685573 | Loss: -0.329033\n",
      "\n",
      "Epoch:    2 | Iter:  121 | Fidelity: 0.691139 | Loss: -0.331241\n",
      "\n",
      "Epoch:    2 | Iter:  131 | Fidelity: 0.695102 | Loss: -0.332807\n",
      "\n",
      "Epoch:    2 | Iter:  141 | Fidelity: 0.697971 | Loss: -0.333937\n",
      "\n",
      "Epoch:    2 | Iter:  151 | Fidelity: 0.700080 | Loss: -0.334764\n",
      "\n",
      "Epoch:    2 | Iter:  161 | Fidelity: 0.701651 | Loss: -0.335386\n",
      "\n",
      "Epoch:    2 | Iter:  171 | Fidelity: 0.702832 | Loss: -0.335845\n",
      "\n",
      "Epoch:    2 | Iter:  181 | Fidelity: 0.703728 | Loss: -0.336190\n",
      "\n",
      "Epoch:    2 | Iter:  191 | Fidelity: 0.704415 | Loss: -0.336461\n",
      "\n",
      "Epoch:    2 | Iter:  201 | Fidelity: 0.704944 | Loss: -0.336659\n",
      "\n",
      "Epoch:    2 | Iter:  211 | Fidelity: 0.705357 | Loss: -0.336825\n",
      "\n",
      "Epoch:    2 | Iter:  221 | Fidelity: 0.705680 | Loss: -0.336946\n",
      "\n",
      "Epoch:    2 | Iter:  231 | Fidelity: 0.705937 | Loss: -0.337049\n",
      "\n",
      "Epoch:    2 | Iter:  241 | Fidelity: 0.706142 | Loss: -0.337119\n",
      "\n",
      "Epoch:    2 | Iter:  251 | Fidelity: 0.706310 | Loss: -0.337180\n",
      "\n",
      "Epoch:    2 | Iter:  261 | Fidelity: 0.706448 | Loss: -0.337236\n",
      "\n",
      "Epoch:    2 | Iter:  271 | Fidelity: 0.706563 | Loss: -0.337276\n",
      "\n",
      "Epoch:    2 | Iter:  281 | Fidelity: 0.706660 | Loss: -0.337317\n",
      "\n",
      "Epoch:    2 | Iter:  291 | Fidelity: 0.706743 | Loss: -0.337347\n",
      "\n",
      "Training finished after 2 epochs.\n",
      "\n",
      "Run took: 42.338925200048834 time.\n",
      "Generator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-gen(hs).pkl\n",
      "Discriminator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-dis(hs).pkl\n"
     ]
    }
   ],
   "source": [
    "training_instance = Training(config=config)\n",
    "res = training_instance.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f87fcb",
   "metadata": {},
   "source": [
    "- Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1132a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator parameters saved to ./models/training_instance_dis.pkl\n",
      "Generator parameters saved to ./models/training_instance_gen.pkl\n",
      "alpha \n",
      " Parameter containing:\n",
      "tensor([[-6.8025e-07,  1.3597e+00,  2.9963e-01,  2.3968e-05],\n",
      "        [ 1.6076e+00,  3.0603e-04,  1.1595e-04, -2.2136e-01],\n",
      "        [-4.7665e-02,  2.7992e-04,  2.3706e-05,  1.6824e+00],\n",
      "        [-3.0627e-06,  3.0993e-01,  1.4065e+00,  7.8263e-06],\n",
      "        [-1.6073e-01, -4.1702e-04,  1.4948e-04, -1.1673e+00],\n",
      "        [-1.5759e+00,  2.8988e-05, -8.9229e-06, -4.4694e-02]],\n",
      "       requires_grad=True)\n",
      "beta \n",
      " Parameter containing:\n",
      "tensor([[-0.9389,  0.7349, -0.5690, -0.5293],\n",
      "        [ 0.2510, -0.6516, -0.4941, -0.2000],\n",
      "        [-0.0548, -0.4536,  0.6523,  0.6736],\n",
      "        [-0.3459,  0.3689, -0.3635, -0.2769],\n",
      "        [-0.7174,  0.3577, -0.0489,  0.3883],\n",
      "        [-0.0299,  0.0250,  0.0028,  0.0017]], requires_grad=True)\n",
      "theta \n",
      " Parameter containing:\n",
      "tensor([-4.2558e-01, -3.1307e+00,  7.4590e-02,  6.1499e-01, -1.8587e+00,\n",
      "        -3.1604e-02,  2.2535e-01, -1.6501e-01,  1.1055e+00, -2.5739e-03,\n",
      "        -7.5310e-02,  4.1156e-01,  1.8839e+00, -4.2504e-01, -9.4647e-02,\n",
      "         1.7097e+00, -1.5215e+00, -1.0494e-02, -4.3888e-04, -1.0251e+00,\n",
      "         1.6038e-01,  4.1116e-01, -2.0269e-01,  1.3831e+00],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "path_dis = \"./models/training_instance_dis.pkl\"\n",
    "path_gen = \"./models/training_instance_gen.pkl\"\n",
    "training_instance.dis.save_model_params(path_dis)\n",
    "training_instance.gen.save_model_params(path_gen)\n",
    "print('alpha \\n', training_instance.dis.alpha)\n",
    "print('beta \\n', training_instance.dis.beta)\n",
    "print('theta \\n', training_instance.gen.ansatz.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd968299",
   "metadata": {},
   "source": [
    "- Load parameters in new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c0c869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator parameters loaded successfully from ./models/training_instance_dis.pkl\n",
      "Generator parameters loaded successfully from ./models/training_instance_gen.pkl\n",
      "alpha \n",
      " Parameter containing:\n",
      "tensor([[-6.8025e-07,  1.3597e+00,  2.9963e-01,  2.3968e-05],\n",
      "        [ 1.6076e+00,  3.0603e-04,  1.1595e-04, -2.2136e-01],\n",
      "        [-4.7665e-02,  2.7992e-04,  2.3706e-05,  1.6824e+00],\n",
      "        [-3.0627e-06,  3.0993e-01,  1.4065e+00,  7.8263e-06],\n",
      "        [-1.6073e-01, -4.1702e-04,  1.4948e-04, -1.1673e+00],\n",
      "        [-1.5759e+00,  2.8988e-05, -8.9229e-06, -4.4694e-02]],\n",
      "       requires_grad=True)\n",
      "beta \n",
      " Parameter containing:\n",
      "tensor([[-0.9389,  0.7349, -0.5690, -0.5293],\n",
      "        [ 0.2510, -0.6516, -0.4941, -0.2000],\n",
      "        [-0.0548, -0.4536,  0.6523,  0.6736],\n",
      "        [-0.3459,  0.3689, -0.3635, -0.2769],\n",
      "        [-0.7174,  0.3577, -0.0489,  0.3883],\n",
      "        [-0.0299,  0.0250,  0.0028,  0.0017]], requires_grad=True)\n",
      "theta \n",
      " Parameter containing:\n",
      "tensor([-4.2558e-01, -3.1307e+00,  7.4590e-02,  6.1499e-01, -1.8587e+00,\n",
      "        -3.1604e-02,  2.2535e-01, -1.6501e-01,  1.1055e+00, -2.5739e-03,\n",
      "        -7.5310e-02,  4.1156e-01,  1.8839e+00, -4.2504e-01, -9.4647e-02,\n",
      "         1.7097e+00, -1.5215e+00, -1.0494e-02, -4.3888e-04, -1.0251e+00,\n",
      "         1.6038e-01,  4.1116e-01, -2.0269e-01,  1.3831e+00],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "path = \"./models/training_instance_dis.pkl\"\n",
    "training_instance2 = Training(config=config)\n",
    "training_instance2.dis.load_model_params(path_dis)\n",
    "training_instance2.gen.load_model_params(path_gen)\n",
    "print('alpha \\n', training_instance2.dis.alpha)\n",
    "print('beta \\n', training_instance2.dis.beta)\n",
    "print('theta \\n', training_instance2.gen.ansatz.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd7b60",
   "metadata": {},
   "source": [
    "- Retrain training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4e1f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== \n",
      "run_timestamp: 2025-11-13__10-04-48,\n",
      "----------------------------------------------\n",
      "load_timestamp: None,\n",
      "type_of_warm_start: none,\n",
      "warm_start_strength: 0.1,\n",
      "----------------------------------------------\n",
      "system_size: 3,\n",
      "extra_ancilla: False,\n",
      "ancilla_mode: pass,\n",
      "ancilla_project_norm: re-norm,\n",
      "ancilla_topology: bridge,\n",
      "ancilla_connect_to: None,\n",
      "do_ancilla_1q_gates: True,\n",
      "start_ancilla_gates_randomly: True,\n",
      "----------------------------------------------\n",
      "gen_layers: 3,\n",
      "gen_ansatz: ZZ_X_Z,\n",
      "----------------------------------------------\n",
      "target_hamiltonian: custom_h,\n",
      "custom_hamiltonian_terms: ['ZZZ'],\n",
      "custom_hamiltonian_strengths: [1.0],\n",
      "----------------------------------------------\n",
      "epochs: 2,\n",
      "iterations_epoch: 300,\n",
      "log_every_x_iter: 10,\n",
      "save_fid_and_loss_every_x_iter: 1,\n",
      "max_fidelity: 0.99,\n",
      "steps_gen: 1,\n",
      "steps_dis: 1,\n",
      "----------------------------------------------\n",
      "l_rate: 0.01,\n",
      "momentum_coeff: 0.9,\n",
      "================================================== \n",
      "\n",
      "\n",
      "Epoch:    1 | Iter:    1 | Fidelity: 0.706809 | Loss: -0.337366\n",
      "\n",
      "Epoch:    1 | Iter:   11 | Fidelity: 0.706839 | Loss: -0.337378\n",
      "\n",
      "Epoch:    1 | Iter:   21 | Fidelity: 0.706888 | Loss: -0.337401\n",
      "\n",
      "Epoch:    1 | Iter:   31 | Fidelity: 0.706940 | Loss: -0.337415\n",
      "\n",
      "Epoch:    1 | Iter:   41 | Fidelity: 0.706990 | Loss: -0.337433\n",
      "\n",
      "Epoch:    1 | Iter:   51 | Fidelity: 0.707036 | Loss: -0.337450\n",
      "\n",
      "Epoch:    1 | Iter:   61 | Fidelity: 0.707079 | Loss: -0.337470\n",
      "\n",
      "Epoch:    1 | Iter:   71 | Fidelity: 0.707119 | Loss: -0.337481\n",
      "\n",
      "Epoch:    1 | Iter:   81 | Fidelity: 0.707155 | Loss: -0.337493\n",
      "\n",
      "Epoch:    1 | Iter:   91 | Fidelity: 0.707190 | Loss: -0.337508\n",
      "\n",
      "Epoch:    1 | Iter:  101 | Fidelity: 0.707222 | Loss: -0.337520\n",
      "\n",
      "Epoch:    1 | Iter:  111 | Fidelity: 0.707253 | Loss: -0.337530\n",
      "\n",
      "Epoch:    1 | Iter:  121 | Fidelity: 0.707282 | Loss: -0.337540\n",
      "\n",
      "Epoch:    1 | Iter:  131 | Fidelity: 0.707309 | Loss: -0.337549\n",
      "\n",
      "Epoch:    1 | Iter:  141 | Fidelity: 0.707335 | Loss: -0.337558\n",
      "\n",
      "Epoch:    1 | Iter:  151 | Fidelity: 0.707360 | Loss: -0.337571\n",
      "\n",
      "Epoch:    1 | Iter:  161 | Fidelity: 0.707383 | Loss: -0.337577\n",
      "\n",
      "Epoch:    1 | Iter:  171 | Fidelity: 0.707406 | Loss: -0.337586\n",
      "\n",
      "Epoch:    1 | Iter:  181 | Fidelity: 0.707427 | Loss: -0.337595\n",
      "\n",
      "Epoch:    1 | Iter:  191 | Fidelity: 0.707448 | Loss: -0.337605\n",
      "\n",
      "Epoch:    1 | Iter:  201 | Fidelity: 0.707467 | Loss: -0.337610\n",
      "\n",
      "Epoch:    1 | Iter:  211 | Fidelity: 0.707486 | Loss: -0.337616\n",
      "\n",
      "Epoch:    1 | Iter:  221 | Fidelity: 0.707504 | Loss: -0.337624\n",
      "\n",
      "Epoch:    1 | Iter:  231 | Fidelity: 0.707521 | Loss: -0.337629\n",
      "\n",
      "Epoch:    1 | Iter:  241 | Fidelity: 0.707538 | Loss: -0.337639\n",
      "\n",
      "Epoch:    1 | Iter:  251 | Fidelity: 0.707554 | Loss: -0.337642\n",
      "\n",
      "Epoch:    1 | Iter:  261 | Fidelity: 0.707569 | Loss: -0.337650\n",
      "\n",
      "Epoch:    1 | Iter:  271 | Fidelity: 0.707584 | Loss: -0.337655\n",
      "\n",
      "Epoch:    1 | Iter:  281 | Fidelity: 0.707598 | Loss: -0.337662\n",
      "\n",
      "Epoch:    1 | Iter:  291 | Fidelity: 0.707612 | Loss: -0.337665\n",
      "\n",
      "Epoch:    2 | Iter:    1 | Fidelity: 0.707625 | Loss: -0.337671\n",
      "\n",
      "Epoch:    2 | Iter:   11 | Fidelity: 0.707637 | Loss: -0.337672\n",
      "\n",
      "Epoch:    2 | Iter:   21 | Fidelity: 0.707649 | Loss: -0.337679\n",
      "\n",
      "Epoch:    2 | Iter:   31 | Fidelity: 0.707661 | Loss: -0.337683\n",
      "\n",
      "Epoch:    2 | Iter:   41 | Fidelity: 0.707672 | Loss: -0.337687\n",
      "\n",
      "Epoch:    2 | Iter:   51 | Fidelity: 0.707683 | Loss: -0.337691\n",
      "\n",
      "Epoch:    2 | Iter:   61 | Fidelity: 0.707694 | Loss: -0.337694\n",
      "\n",
      "Epoch:    2 | Iter:   71 | Fidelity: 0.707704 | Loss: -0.337698\n",
      "\n",
      "Epoch:    2 | Iter:   81 | Fidelity: 0.707713 | Loss: -0.337702\n",
      "\n",
      "Epoch:    2 | Iter:   91 | Fidelity: 0.707723 | Loss: -0.337707\n",
      "\n",
      "Epoch:    2 | Iter:  101 | Fidelity: 0.707732 | Loss: -0.337710\n",
      "\n",
      "Epoch:    2 | Iter:  111 | Fidelity: 0.707741 | Loss: -0.337710\n",
      "\n",
      "Epoch:    2 | Iter:  121 | Fidelity: 0.707749 | Loss: -0.337714\n",
      "\n",
      "Epoch:    2 | Iter:  131 | Fidelity: 0.707757 | Loss: -0.337717\n",
      "\n",
      "Epoch:    2 | Iter:  141 | Fidelity: 0.707765 | Loss: -0.337717\n",
      "\n",
      "Epoch:    2 | Iter:  151 | Fidelity: 0.707773 | Loss: -0.337723\n",
      "\n",
      "Epoch:    2 | Iter:  161 | Fidelity: 0.707781 | Loss: -0.337724\n",
      "\n",
      "Epoch:    2 | Iter:  171 | Fidelity: 0.707788 | Loss: -0.337725\n",
      "\n",
      "Epoch:    2 | Iter:  181 | Fidelity: 0.707795 | Loss: -0.337729\n",
      "\n",
      "Epoch:    2 | Iter:  191 | Fidelity: 0.707802 | Loss: -0.337733\n",
      "\n",
      "Epoch:    2 | Iter:  201 | Fidelity: 0.707808 | Loss: -0.337732\n",
      "\n",
      "Epoch:    2 | Iter:  211 | Fidelity: 0.707814 | Loss: -0.337731\n",
      "\n",
      "Epoch:    2 | Iter:  221 | Fidelity: 0.707820 | Loss: -0.337734\n",
      "\n",
      "Epoch:    2 | Iter:  231 | Fidelity: 0.707826 | Loss: -0.337735\n",
      "\n",
      "Epoch:    2 | Iter:  241 | Fidelity: 0.707832 | Loss: -0.337737\n",
      "\n",
      "Epoch:    2 | Iter:  251 | Fidelity: 0.707838 | Loss: -0.337740\n",
      "\n",
      "Epoch:    2 | Iter:  261 | Fidelity: 0.707843 | Loss: -0.337744\n",
      "\n",
      "Epoch:    2 | Iter:  271 | Fidelity: 0.707849 | Loss: -0.337747\n",
      "\n",
      "Epoch:    2 | Iter:  281 | Fidelity: 0.707853 | Loss: -0.337746\n",
      "\n",
      "Epoch:    2 | Iter:  291 | Fidelity: 0.707859 | Loss: -0.337746\n",
      "\n",
      "Training finished after 2 epochs.\n",
      "\n",
      "Run took: 41.68987920007203 time.\n",
      "Generator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-gen(hs).pkl\n",
      "Discriminator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-dis(hs).pkl\n"
     ]
    }
   ],
   "source": [
    "res2 = training_instance2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532f12a",
   "metadata": {},
   "source": [
    "## 3. Generator and discriminator with extra ancilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910358fc",
   "metadata": {},
   "source": [
    "We want to add an ancilla and keep training the circuit from where we stopped. For that, we will load the parameters and add zeroes to the new added parameters. For the discriminator and generator, the ancilla corresponds to the last qubit, then meaning that we need to add the parameters at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf39b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qgan.training import Training\n",
    "from config import CFG\n",
    "from tools.data.data_managers import save_model\n",
    "import os \n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68b6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = copy.deepcopy(CFG)\n",
    "config.epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ff2dc",
   "metadata": {},
   "source": [
    "- Run training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b15cb2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== \n",
      "run_timestamp: 2025-11-13__10-04-48,\n",
      "----------------------------------------------\n",
      "load_timestamp: None,\n",
      "type_of_warm_start: none,\n",
      "warm_start_strength: 0.1,\n",
      "----------------------------------------------\n",
      "system_size: 3,\n",
      "extra_ancilla: False,\n",
      "ancilla_mode: pass,\n",
      "ancilla_project_norm: re-norm,\n",
      "ancilla_topology: bridge,\n",
      "ancilla_connect_to: None,\n",
      "do_ancilla_1q_gates: True,\n",
      "start_ancilla_gates_randomly: True,\n",
      "----------------------------------------------\n",
      "gen_layers: 3,\n",
      "gen_ansatz: ZZ_X_Z,\n",
      "----------------------------------------------\n",
      "target_hamiltonian: custom_h,\n",
      "custom_hamiltonian_terms: ['ZZZ'],\n",
      "custom_hamiltonian_strengths: [1.0],\n",
      "----------------------------------------------\n",
      "epochs: 2,\n",
      "iterations_epoch: 300,\n",
      "log_every_x_iter: 10,\n",
      "save_fid_and_loss_every_x_iter: 1,\n",
      "max_fidelity: 0.99,\n",
      "steps_gen: 1,\n",
      "steps_dis: 1,\n",
      "----------------------------------------------\n",
      "l_rate: 0.01,\n",
      "momentum_coeff: 0.9,\n",
      "================================================== \n",
      "\n",
      "\n",
      "Epoch:    1 | Iter:    1 | Fidelity: 0.011073 | Loss: -3.233614\n",
      "\n",
      "Epoch:    1 | Iter:   11 | Fidelity: 0.150648 | Loss: -3.791093\n",
      "\n",
      "Epoch:    1 | Iter:   21 | Fidelity: 0.269457 | Loss: -3.467687\n",
      "\n",
      "Epoch:    1 | Iter:   31 | Fidelity: 0.207259 | Loss: -3.229568\n",
      "\n",
      "Epoch:    1 | Iter:   41 | Fidelity: 0.136406 | Loss: -2.825688\n",
      "\n",
      "Epoch:    1 | Iter:   51 | Fidelity: 0.055104 | Loss: -2.380002\n",
      "\n",
      "Epoch:    1 | Iter:   61 | Fidelity: 0.055624 | Loss: -0.933475\n",
      "\n",
      "Epoch:    1 | Iter:   71 | Fidelity: 0.096130 | Loss: 0.129684\n",
      "\n",
      "Epoch:    1 | Iter:   81 | Fidelity: 0.122304 | Loss: 0.335330\n",
      "\n",
      "Epoch:    1 | Iter:   91 | Fidelity: 0.146286 | Loss: 0.391003\n",
      "\n",
      "Epoch:    1 | Iter:  101 | Fidelity: 0.174437 | Loss: 0.392993\n",
      "\n",
      "Epoch:    1 | Iter:  111 | Fidelity: 0.206793 | Loss: 0.381368\n",
      "\n",
      "Epoch:    1 | Iter:  121 | Fidelity: 0.242182 | Loss: 0.367711\n",
      "\n",
      "Epoch:    1 | Iter:  131 | Fidelity: 0.278604 | Loss: 0.350574\n",
      "\n",
      "Epoch:    1 | Iter:  141 | Fidelity: 0.313378 | Loss: 0.333442\n",
      "\n",
      "Epoch:    1 | Iter:  151 | Fidelity: 0.344186 | Loss: 0.318402\n",
      "\n",
      "Epoch:    1 | Iter:  161 | Fidelity: 0.369820 | Loss: 0.305779\n",
      "\n",
      "Epoch:    1 | Iter:  171 | Fidelity: 0.390289 | Loss: 0.295707\n",
      "\n",
      "Epoch:    1 | Iter:  181 | Fidelity: 0.406347 | Loss: 0.287824\n",
      "\n",
      "Epoch:    1 | Iter:  191 | Fidelity: 0.418938 | Loss: 0.281664\n",
      "\n",
      "Epoch:    1 | Iter:  201 | Fidelity: 0.428892 | Loss: 0.276814\n",
      "\n",
      "Epoch:    1 | Iter:  211 | Fidelity: 0.436839 | Loss: 0.272961\n",
      "\n",
      "Epoch:    1 | Iter:  221 | Fidelity: 0.443232 | Loss: 0.269875\n",
      "\n",
      "Epoch:    1 | Iter:  231 | Fidelity: 0.448405 | Loss: 0.267393\n",
      "\n",
      "Epoch:    1 | Iter:  241 | Fidelity: 0.452610 | Loss: 0.265387\n",
      "\n",
      "Epoch:    1 | Iter:  251 | Fidelity: 0.456043 | Loss: 0.263755\n",
      "\n",
      "Epoch:    1 | Iter:  261 | Fidelity: 0.458859 | Loss: 0.262420\n",
      "\n",
      "Epoch:    1 | Iter:  271 | Fidelity: 0.461178 | Loss: 0.261338\n",
      "\n",
      "Epoch:    1 | Iter:  281 | Fidelity: 0.463101 | Loss: 0.260439\n",
      "\n",
      "Epoch:    1 | Iter:  291 | Fidelity: 0.464703 | Loss: 0.259702\n",
      "\n",
      "Epoch:    2 | Iter:    1 | Fidelity: 0.466046 | Loss: 0.259083\n",
      "\n",
      "Epoch:    2 | Iter:   11 | Fidelity: 0.467179 | Loss: 0.258569\n",
      "\n",
      "Epoch:    2 | Iter:   21 | Fidelity: 0.468142 | Loss: 0.258134\n",
      "\n",
      "Epoch:    2 | Iter:   31 | Fidelity: 0.468966 | Loss: 0.257767\n",
      "\n",
      "Epoch:    2 | Iter:   41 | Fidelity: 0.469676 | Loss: 0.257453\n",
      "\n",
      "Epoch:    2 | Iter:   51 | Fidelity: 0.470291 | Loss: 0.257179\n",
      "\n",
      "Epoch:    2 | Iter:   61 | Fidelity: 0.470829 | Loss: 0.256947\n",
      "\n",
      "Epoch:    2 | Iter:   71 | Fidelity: 0.471301 | Loss: 0.256743\n",
      "\n",
      "Epoch:    2 | Iter:   81 | Fidelity: 0.471718 | Loss: 0.256570\n",
      "\n",
      "Epoch:    2 | Iter:   91 | Fidelity: 0.472090 | Loss: 0.256408\n",
      "\n",
      "Epoch:    2 | Iter:  101 | Fidelity: 0.472423 | Loss: 0.256269\n",
      "\n",
      "Epoch:    2 | Iter:  111 | Fidelity: 0.472722 | Loss: 0.256148\n",
      "\n",
      "Epoch:    2 | Iter:  121 | Fidelity: 0.472992 | Loss: 0.256039\n",
      "\n",
      "Epoch:    2 | Iter:  131 | Fidelity: 0.473237 | Loss: 0.255937\n",
      "\n",
      "Epoch:    2 | Iter:  141 | Fidelity: 0.473461 | Loss: 0.255849\n",
      "\n",
      "Epoch:    2 | Iter:  151 | Fidelity: 0.473666 | Loss: 0.255760\n",
      "\n",
      "Epoch:    2 | Iter:  161 | Fidelity: 0.473854 | Loss: 0.255689\n",
      "\n",
      "Epoch:    2 | Iter:  171 | Fidelity: 0.474027 | Loss: 0.255623\n",
      "\n",
      "Epoch:    2 | Iter:  181 | Fidelity: 0.474186 | Loss: 0.255567\n",
      "\n",
      "Epoch:    2 | Iter:  191 | Fidelity: 0.474335 | Loss: 0.255501\n",
      "\n",
      "Epoch:    2 | Iter:  201 | Fidelity: 0.474472 | Loss: 0.255449\n",
      "\n",
      "Epoch:    2 | Iter:  211 | Fidelity: 0.474601 | Loss: 0.255402\n",
      "\n",
      "Epoch:    2 | Iter:  221 | Fidelity: 0.474720 | Loss: 0.255353\n",
      "\n",
      "Epoch:    2 | Iter:  231 | Fidelity: 0.474832 | Loss: 0.255315\n",
      "\n",
      "Epoch:    2 | Iter:  241 | Fidelity: 0.474936 | Loss: 0.255276\n",
      "\n",
      "Epoch:    2 | Iter:  251 | Fidelity: 0.475034 | Loss: 0.255242\n",
      "\n",
      "Epoch:    2 | Iter:  261 | Fidelity: 0.475126 | Loss: 0.255208\n",
      "\n",
      "Epoch:    2 | Iter:  271 | Fidelity: 0.475213 | Loss: 0.255176\n",
      "\n",
      "Epoch:    2 | Iter:  281 | Fidelity: 0.475294 | Loss: 0.255145\n",
      "\n",
      "Epoch:    2 | Iter:  291 | Fidelity: 0.475371 | Loss: 0.255117\n",
      "\n",
      "Training finished after 2 epochs.\n",
      "\n",
      "Run took: 37.84960980003234 time.\n",
      "Generator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-gen(hs).pkl\n",
      "Discriminator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-dis(hs).pkl\n"
     ]
    }
   ],
   "source": [
    "training_instance = Training(config=config)\n",
    "res = training_instance.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b36cac",
   "metadata": {},
   "source": [
    "- Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8627bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator parameters saved to ./models/training_instance_dis_ancilla.pkl\n",
      "Generator parameters saved to ./models/training_instance_gen_ancilla.pkl\n",
      "alpha \n",
      " Parameter containing:\n",
      "tensor([[-5.3661e-02,  2.4967e-04, -2.1447e-04, -1.2534e+00],\n",
      "        [-1.7704e-05, -1.4841e+00,  1.4637e+00,  4.9753e-05],\n",
      "        [ 5.6801e-07, -1.1521e+00,  4.5160e-01, -2.0520e-04],\n",
      "        [-5.1663e-02, -1.0483e-03,  4.0130e-04,  1.2072e+00],\n",
      "        [-3.9875e-05,  1.2911e+00,  1.2735e+00,  3.0572e-05],\n",
      "        [ 6.9994e-08, -1.3505e+00, -5.2936e-01,  2.0412e-04]],\n",
      "       requires_grad=True)\n",
      "beta \n",
      " Parameter containing:\n",
      "tensor([[-0.4280, -0.0965, -0.1510, -0.0606],\n",
      "        [ 0.2507, -0.5719, -0.7123, -0.0706],\n",
      "        [ 0.0308,  0.5086, -0.2775,  0.1050],\n",
      "        [-0.4648,  0.0162, -0.2931, -0.4282],\n",
      "        [-0.0488,  0.1855,  0.2281, -0.4232],\n",
      "        [-0.0033, -0.5522, -0.3167,  0.0994]], requires_grad=True)\n",
      "theta \n",
      " Parameter containing:\n",
      "tensor([ 2.8790e-02,  1.6353e+00, -2.5112e-03, -3.6913e-01, -2.2455e-02,\n",
      "        -1.4477e-01,  1.5597e+00,  1.4107e+00, -3.5741e-01,  3.0831e-01,\n",
      "        -9.8291e-04, -1.2201e+00,  1.2614e-02,  1.1377e+00,  3.0043e+00,\n",
      "         1.5435e+00, -3.5078e-01, -3.6587e-01,  1.1847e-03,  1.6120e-02,\n",
      "        -2.3326e-02,  5.7945e-01,  1.4716e-01,  1.7588e+00],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "path_dis = \"./models/training_instance_dis_ancilla.pkl\"\n",
    "path_gen = \"./models/training_instance_gen_ancilla.pkl\"\n",
    "training_instance.dis.save_model_params(path_dis)\n",
    "training_instance.gen.save_model_params(path_gen)\n",
    "print('alpha \\n', training_instance.dis.alpha)\n",
    "print('beta \\n', training_instance.dis.beta)\n",
    "print('theta \\n', training_instance.gen.ansatz.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0ddfc",
   "metadata": {},
   "source": [
    "- Load parameters in new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad7c3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = copy.deepcopy(CFG)\n",
    "config2.extra_ancilla = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e503838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding alpha: 6 â†’ 7\n",
      "Expanding beta: 6 â†’ 7\n",
      "Discriminator parameters loaded successfully from ./models/training_instance_dis_ancilla.pkl\n",
      "Expanding Î¸: 24 â†’ 36\n",
      "alpha \n",
      " Parameter containing:\n",
      "tensor([[-5.3661e-02,  2.4967e-04, -2.1447e-04, -1.2534e+00],\n",
      "        [-1.7704e-05, -1.4841e+00,  1.4637e+00,  4.9753e-05],\n",
      "        [ 5.6801e-07, -1.1521e+00,  4.5160e-01, -2.0520e-04],\n",
      "        [-5.1663e-02, -1.0483e-03,  4.0130e-04,  1.2072e+00],\n",
      "        [-3.9875e-05,  1.2911e+00,  1.2735e+00,  3.0572e-05],\n",
      "        [ 6.9994e-08, -1.3505e+00, -5.2936e-01,  2.0412e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       requires_grad=True)\n",
      "beta \n",
      " Parameter containing:\n",
      "tensor([[-0.4280, -0.0965, -0.1510, -0.0606],\n",
      "        [ 0.2507, -0.5719, -0.7123, -0.0706],\n",
      "        [ 0.0308,  0.5086, -0.2775,  0.1050],\n",
      "        [-0.4648,  0.0162, -0.2931, -0.4282],\n",
      "        [-0.0488,  0.1855,  0.2281, -0.4232],\n",
      "        [-0.0033, -0.5522, -0.3167,  0.0994],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], requires_grad=True)\n",
      "theta \n",
      " Parameter containing:\n",
      "tensor([ 2.8790e-02,  1.6353e+00, -2.5112e-03, -3.6913e-01, -2.2455e-02,\n",
      "        -1.4477e-01,  1.5597e+00,  1.4107e+00, -3.5741e-01,  3.0831e-01,\n",
      "        -9.8291e-04, -1.2201e+00,  1.2614e-02,  1.1377e+00,  3.0043e+00,\n",
      "         1.5435e+00, -3.5078e-01, -3.6587e-01,  1.1847e-03,  1.6120e-02,\n",
      "        -2.3326e-02,  5.7945e-01,  1.4716e-01,  1.7588e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "path = \"./models/training_instance_dis.pkl\"\n",
    "training_instance2 = Training(config=config2)\n",
    "training_instance2.dis.load_model_params(path_dis)\n",
    "training_instance2.gen.load_model_params(path_gen)\n",
    "print('alpha \\n', training_instance2.dis.alpha)\n",
    "print('beta \\n', training_instance2.dis.beta)\n",
    "print('theta \\n', training_instance2.gen.ansatz.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4c7c2",
   "metadata": {},
   "source": [
    "- Retrain training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a1663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== \n",
      "run_timestamp: 2025-11-13__10-04-48,\n",
      "----------------------------------------------\n",
      "load_timestamp: None,\n",
      "type_of_warm_start: none,\n",
      "warm_start_strength: 0.1,\n",
      "----------------------------------------------\n",
      "system_size: 3,\n",
      "extra_ancilla: True,\n",
      "ancilla_mode: pass,\n",
      "ancilla_project_norm: re-norm,\n",
      "ancilla_topology: bridge,\n",
      "ancilla_connect_to: None,\n",
      "do_ancilla_1q_gates: True,\n",
      "start_ancilla_gates_randomly: True,\n",
      "----------------------------------------------\n",
      "gen_layers: 3,\n",
      "gen_ansatz: ZZ_X_Z,\n",
      "----------------------------------------------\n",
      "target_hamiltonian: custom_h,\n",
      "custom_hamiltonian_terms: ['ZZZ'],\n",
      "custom_hamiltonian_strengths: [1.0],\n",
      "----------------------------------------------\n",
      "epochs: 10,\n",
      "iterations_epoch: 300,\n",
      "log_every_x_iter: 10,\n",
      "save_fid_and_loss_every_x_iter: 1,\n",
      "max_fidelity: 0.99,\n",
      "steps_gen: 1,\n",
      "steps_dis: 1,\n",
      "----------------------------------------------\n",
      "l_rate: 0.01,\n",
      "momentum_coeff: 0.9,\n",
      "================================================== \n",
      "\n",
      "\n",
      "Epoch:    1 | Iter:    1 | Fidelity: 0.000474 | Loss: -2.651219\n",
      "\n",
      "Epoch:    1 | Iter:   11 | Fidelity: 0.000489 | Loss: -0.471320\n",
      "\n",
      "Epoch:    1 | Iter:   21 | Fidelity: 0.000518 | Loss: 0.238004\n",
      "\n",
      "Epoch:    1 | Iter:   31 | Fidelity: 0.000555 | Loss: 0.485353\n",
      "\n",
      "Epoch:    1 | Iter:   41 | Fidelity: 0.000598 | Loss: 0.464311\n",
      "\n",
      "Epoch:    1 | Iter:   51 | Fidelity: 0.000646 | Loss: 0.473900\n",
      "\n",
      "Epoch:    1 | Iter:   61 | Fidelity: 0.000699 | Loss: 0.489211\n",
      "\n",
      "Epoch:    1 | Iter:   71 | Fidelity: 0.000756 | Loss: 0.489335\n",
      "\n",
      "Epoch:    1 | Iter:   81 | Fidelity: 0.000818 | Loss: 0.489164\n",
      "\n",
      "Epoch:    1 | Iter:   91 | Fidelity: 0.000886 | Loss: 0.489725\n",
      "\n",
      "Epoch:    1 | Iter:  101 | Fidelity: 0.000959 | Loss: 0.489790\n",
      "\n",
      "Epoch:    1 | Iter:  111 | Fidelity: 0.001038 | Loss: 0.489726\n",
      "\n",
      "Epoch:    1 | Iter:  121 | Fidelity: 0.001124 | Loss: 0.489697\n",
      "\n",
      "Epoch:    1 | Iter:  131 | Fidelity: 0.001218 | Loss: 0.489649\n",
      "\n",
      "Epoch:    1 | Iter:  141 | Fidelity: 0.001320 | Loss: 0.489591\n",
      "\n",
      "Epoch:    1 | Iter:  151 | Fidelity: 0.001430 | Loss: 0.489531\n",
      "\n",
      "Epoch:    1 | Iter:  161 | Fidelity: 0.001551 | Loss: 0.489468\n",
      "\n",
      "Epoch:    1 | Iter:  171 | Fidelity: 0.001682 | Loss: 0.489396\n",
      "\n",
      "Epoch:    1 | Iter:  181 | Fidelity: 0.001826 | Loss: 0.489320\n",
      "\n",
      "Epoch:    1 | Iter:  191 | Fidelity: 0.001982 | Loss: 0.489232\n",
      "\n",
      "Epoch:    1 | Iter:  201 | Fidelity: 0.002154 | Loss: 0.489146\n",
      "\n",
      "Epoch:    1 | Iter:  211 | Fidelity: 0.002342 | Loss: 0.489051\n",
      "\n",
      "Epoch:    1 | Iter:  221 | Fidelity: 0.002548 | Loss: 0.488945\n",
      "\n",
      "Epoch:    1 | Iter:  231 | Fidelity: 0.002775 | Loss: 0.488830\n",
      "\n",
      "Epoch:    1 | Iter:  241 | Fidelity: 0.003025 | Loss: 0.488699\n",
      "\n",
      "Epoch:    1 | Iter:  251 | Fidelity: 0.003301 | Loss: 0.488559\n",
      "\n",
      "Epoch:    1 | Iter:  261 | Fidelity: 0.003608 | Loss: 0.488406\n",
      "\n",
      "Epoch:    1 | Iter:  271 | Fidelity: 0.003948 | Loss: 0.488243\n",
      "\n",
      "Epoch:    1 | Iter:  281 | Fidelity: 0.004326 | Loss: 0.488051\n",
      "\n",
      "Epoch:    1 | Iter:  291 | Fidelity: 0.004750 | Loss: 0.487843\n",
      "\n",
      "Epoch:    2 | Iter:    1 | Fidelity: 0.005224 | Loss: 0.487608\n",
      "\n",
      "Epoch:    2 | Iter:   11 | Fidelity: 0.005758 | Loss: 0.487347\n",
      "\n",
      "Epoch:    2 | Iter:   21 | Fidelity: 0.006362 | Loss: 0.487051\n",
      "\n",
      "Epoch:    2 | Iter:   31 | Fidelity: 0.007046 | Loss: 0.486716\n",
      "\n",
      "Epoch:    2 | Iter:   41 | Fidelity: 0.007827 | Loss: 0.486338\n",
      "\n",
      "Epoch:    2 | Iter:   51 | Fidelity: 0.008720 | Loss: 0.485903\n",
      "\n",
      "Epoch:    2 | Iter:   61 | Fidelity: 0.009749 | Loss: 0.485397\n",
      "\n",
      "Epoch:    2 | Iter:   71 | Fidelity: 0.010940 | Loss: 0.484821\n",
      "\n",
      "Epoch:    2 | Iter:   81 | Fidelity: 0.012326 | Loss: 0.484148\n",
      "\n",
      "Epoch:    2 | Iter:   91 | Fidelity: 0.013950 | Loss: 0.483358\n",
      "\n",
      "Epoch:    2 | Iter:  101 | Fidelity: 0.015864 | Loss: 0.482429\n",
      "\n",
      "Epoch:    2 | Iter:  111 | Fidelity: 0.018137 | Loss: 0.481318\n",
      "\n",
      "Epoch:    2 | Iter:  121 | Fidelity: 0.020855 | Loss: 0.479999\n",
      "\n",
      "Epoch:    2 | Iter:  131 | Fidelity: 0.024126 | Loss: 0.478406\n",
      "\n",
      "Epoch:    2 | Iter:  141 | Fidelity: 0.028095 | Loss: 0.476469\n",
      "\n",
      "Epoch:    2 | Iter:  151 | Fidelity: 0.032944 | Loss: 0.474096\n",
      "\n",
      "Epoch:    2 | Iter:  161 | Fidelity: 0.038909 | Loss: 0.471173\n",
      "\n",
      "Epoch:    2 | Iter:  171 | Fidelity: 0.046296 | Loss: 0.467549\n",
      "\n",
      "Epoch:    2 | Iter:  181 | Fidelity: 0.055492 | Loss: 0.463034\n",
      "\n",
      "Epoch:    2 | Iter:  191 | Fidelity: 0.066982 | Loss: 0.457375\n",
      "\n",
      "Epoch:    2 | Iter:  201 | Fidelity: 0.081363 | Loss: 0.450280\n",
      "\n",
      "Epoch:    2 | Iter:  211 | Fidelity: 0.099330 | Loss: 0.441397\n",
      "\n",
      "Epoch:    2 | Iter:  221 | Fidelity: 0.121650 | Loss: 0.430346\n",
      "\n",
      "Epoch:    2 | Iter:  231 | Fidelity: 0.149077 | Loss: 0.416738\n",
      "\n",
      "Epoch:    2 | Iter:  241 | Fidelity: 0.182212 | Loss: 0.400277\n",
      "\n",
      "Epoch:    2 | Iter:  251 | Fidelity: 0.221326 | Loss: 0.380829\n",
      "\n",
      "Epoch:    2 | Iter:  261 | Fidelity: 0.266167 | Loss: 0.358528\n",
      "\n",
      "Epoch:    2 | Iter:  271 | Fidelity: 0.315858 | Loss: 0.333824\n",
      "\n",
      "Epoch:    2 | Iter:  281 | Fidelity: 0.368899 | Loss: 0.307479\n",
      "\n",
      "Epoch:    2 | Iter:  291 | Fidelity: 0.423205 | Loss: 0.280522\n",
      "\n",
      "Epoch:    3 | Iter:    1 | Fidelity: 0.476167 | Loss: 0.254277\n",
      "\n",
      "Epoch:    3 | Iter:   11 | Fidelity: 0.525080 | Loss: 0.230065\n",
      "\n",
      "Epoch:    3 | Iter:   21 | Fidelity: 0.568083 | Loss: 0.208811\n",
      "\n",
      "Epoch:    3 | Iter:   31 | Fidelity: 0.604629 | Loss: 0.190784\n",
      "\n",
      "Epoch:    3 | Iter:   41 | Fidelity: 0.634804 | Loss: 0.175932\n",
      "\n",
      "Epoch:    3 | Iter:   51 | Fidelity: 0.658672 | Loss: 0.164195\n",
      "\n",
      "Epoch:    3 | Iter:   61 | Fidelity: 0.676547 | Loss: 0.155422\n",
      "\n",
      "Epoch:    3 | Iter:   71 | Fidelity: 0.689300 | Loss: 0.149173\n",
      "\n",
      "Epoch:    3 | Iter:   81 | Fidelity: 0.698116 | Loss: 0.144856\n",
      "\n",
      "Epoch:    3 | Iter:   91 | Fidelity: 0.704126 | Loss: 0.141916\n",
      "\n",
      "Epoch:    3 | Iter:  101 | Fidelity: 0.708230 | Loss: 0.139919\n",
      "\n",
      "Epoch:    3 | Iter:  111 | Fidelity: 0.711086 | Loss: 0.138521\n",
      "\n",
      "Epoch:    3 | Iter:  121 | Fidelity: 0.713138 | Loss: 0.137525\n",
      "\n",
      "Epoch:    3 | Iter:  131 | Fidelity: 0.714678 | Loss: 0.136777\n",
      "\n",
      "Epoch:    3 | Iter:  141 | Fidelity: 0.715892 | Loss: 0.136185\n",
      "\n",
      "Epoch:    3 | Iter:  151 | Fidelity: 0.716897 | Loss: 0.135701\n",
      "\n",
      "Epoch:    3 | Iter:  161 | Fidelity: 0.717770 | Loss: 0.135282\n",
      "\n",
      "Epoch:    3 | Iter:  171 | Fidelity: 0.718558 | Loss: 0.134901\n",
      "\n",
      "Epoch:    3 | Iter:  181 | Fidelity: 0.719291 | Loss: 0.134549\n",
      "\n",
      "Epoch:    3 | Iter:  191 | Fidelity: 0.719990 | Loss: 0.134215\n",
      "\n",
      "Epoch:    3 | Iter:  201 | Fidelity: 0.720669 | Loss: 0.133887\n",
      "\n",
      "Epoch:    3 | Iter:  211 | Fidelity: 0.721337 | Loss: 0.133563\n",
      "\n",
      "Epoch:    3 | Iter:  221 | Fidelity: 0.722000 | Loss: 0.133248\n",
      "\n",
      "Epoch:    3 | Iter:  231 | Fidelity: 0.722665 | Loss: 0.132928\n",
      "\n",
      "Epoch:    3 | Iter:  241 | Fidelity: 0.723335 | Loss: 0.132611\n",
      "\n",
      "Epoch:    3 | Iter:  251 | Fidelity: 0.724015 | Loss: 0.132286\n",
      "\n",
      "Epoch:    3 | Iter:  261 | Fidelity: 0.724707 | Loss: 0.131957\n",
      "\n",
      "Epoch:    3 | Iter:  271 | Fidelity: 0.725415 | Loss: 0.131622\n",
      "\n",
      "Epoch:    3 | Iter:  281 | Fidelity: 0.726140 | Loss: 0.131273\n",
      "\n",
      "Epoch:    3 | Iter:  291 | Fidelity: 0.726886 | Loss: 0.130918\n",
      "\n",
      "Epoch:    4 | Iter:    1 | Fidelity: 0.727656 | Loss: 0.130551\n",
      "\n",
      "Epoch:    4 | Iter:   11 | Fidelity: 0.728450 | Loss: 0.130172\n",
      "\n",
      "Epoch:    4 | Iter:   21 | Fidelity: 0.729272 | Loss: 0.129783\n",
      "\n",
      "Epoch:    4 | Iter:   31 | Fidelity: 0.730126 | Loss: 0.129369\n",
      "\n",
      "Epoch:    4 | Iter:   41 | Fidelity: 0.731011 | Loss: 0.128948\n",
      "\n",
      "Epoch:    4 | Iter:   51 | Fidelity: 0.731932 | Loss: 0.128514\n",
      "\n",
      "Epoch:    4 | Iter:   61 | Fidelity: 0.732891 | Loss: 0.128060\n",
      "\n",
      "Epoch:    4 | Iter:   71 | Fidelity: 0.733893 | Loss: 0.127580\n",
      "\n",
      "Epoch:    4 | Iter:   81 | Fidelity: 0.734938 | Loss: 0.127081\n",
      "\n",
      "Epoch:    4 | Iter:   91 | Fidelity: 0.736030 | Loss: 0.126557\n",
      "\n",
      "Epoch:    4 | Iter:  101 | Fidelity: 0.737173 | Loss: 0.126018\n",
      "\n",
      "Epoch:    4 | Iter:  111 | Fidelity: 0.738372 | Loss: 0.125445\n",
      "\n",
      "Epoch:    4 | Iter:  121 | Fidelity: 0.739629 | Loss: 0.124846\n",
      "\n",
      "Epoch:    4 | Iter:  131 | Fidelity: 0.740949 | Loss: 0.124219\n",
      "\n",
      "Epoch:    4 | Iter:  141 | Fidelity: 0.742336 | Loss: 0.123557\n",
      "\n",
      "Epoch:    4 | Iter:  151 | Fidelity: 0.743797 | Loss: 0.122859\n",
      "\n",
      "Epoch:    4 | Iter:  161 | Fidelity: 0.745336 | Loss: 0.122124\n",
      "\n",
      "Epoch:    4 | Iter:  171 | Fidelity: 0.746958 | Loss: 0.121352\n",
      "\n",
      "Epoch:    4 | Iter:  181 | Fidelity: 0.748672 | Loss: 0.120534\n",
      "\n",
      "Epoch:    4 | Iter:  191 | Fidelity: 0.750484 | Loss: 0.119670\n",
      "\n",
      "Epoch:    4 | Iter:  201 | Fidelity: 0.752400 | Loss: 0.118756\n",
      "\n",
      "Epoch:    4 | Iter:  211 | Fidelity: 0.754431 | Loss: 0.117786\n",
      "\n",
      "Epoch:    4 | Iter:  221 | Fidelity: 0.756584 | Loss: 0.116756\n",
      "\n",
      "Epoch:    4 | Iter:  231 | Fidelity: 0.758871 | Loss: 0.115664\n",
      "\n",
      "Epoch:    4 | Iter:  241 | Fidelity: 0.761302 | Loss: 0.114495\n",
      "\n",
      "Epoch:    4 | Iter:  251 | Fidelity: 0.763887 | Loss: 0.113268\n",
      "\n",
      "Epoch:    4 | Iter:  261 | Fidelity: 0.766640 | Loss: 0.111954\n",
      "\n",
      "Epoch:    4 | Iter:  271 | Fidelity: 0.769576 | Loss: 0.110550\n",
      "\n",
      "Epoch:    4 | Iter:  281 | Fidelity: 0.772708 | Loss: 0.109047\n",
      "\n",
      "Epoch:    4 | Iter:  291 | Fidelity: 0.776054 | Loss: 0.107444\n",
      "\n",
      "Epoch:    5 | Iter:    1 | Fidelity: 0.779629 | Loss: 0.105730\n",
      "\n",
      "Epoch:    5 | Iter:   11 | Fidelity: 0.783452 | Loss: 0.103894\n",
      "\n",
      "Epoch:    5 | Iter:   21 | Fidelity: 0.787543 | Loss: 0.101941\n",
      "\n",
      "Epoch:    5 | Iter:   31 | Fidelity: 0.791922 | Loss: 0.099836\n",
      "\n",
      "Epoch:    5 | Iter:   41 | Fidelity: 0.796611 | Loss: 0.097587\n",
      "\n",
      "Epoch:    5 | Iter:   51 | Fidelity: 0.801632 | Loss: 0.095180\n",
      "\n",
      "Epoch:    5 | Iter:   61 | Fidelity: 0.807007 | Loss: 0.092601\n",
      "\n",
      "Epoch:    5 | Iter:   71 | Fidelity: 0.812757 | Loss: 0.089844\n",
      "\n",
      "Epoch:    5 | Iter:   81 | Fidelity: 0.818906 | Loss: 0.086897\n",
      "\n",
      "Epoch:    5 | Iter:   91 | Fidelity: 0.825472 | Loss: 0.083741\n",
      "\n",
      "Epoch:    5 | Iter:  101 | Fidelity: 0.832471 | Loss: 0.080379\n",
      "\n",
      "Epoch:    5 | Iter:  111 | Fidelity: 0.839913 | Loss: 0.076804\n",
      "\n",
      "Epoch:    5 | Iter:  121 | Fidelity: 0.847803 | Loss: 0.073025\n",
      "\n",
      "Epoch:    5 | Iter:  131 | Fidelity: 0.856136 | Loss: 0.069018\n",
      "\n",
      "Epoch:    5 | Iter:  141 | Fidelity: 0.864893 | Loss: 0.064815\n",
      "\n",
      "Epoch:    5 | Iter:  151 | Fidelity: 0.874043 | Loss: 0.060419\n",
      "\n",
      "Epoch:    5 | Iter:  161 | Fidelity: 0.883533 | Loss: 0.055859\n",
      "\n",
      "Epoch:    5 | Iter:  171 | Fidelity: 0.893291 | Loss: 0.051169\n",
      "\n",
      "Epoch:    5 | Iter:  181 | Fidelity: 0.903221 | Loss: 0.046405\n",
      "\n",
      "Epoch:    5 | Iter:  191 | Fidelity: 0.913208 | Loss: 0.041618\n",
      "\n",
      "Epoch:    5 | Iter:  201 | Fidelity: 0.923115 | Loss: 0.036865\n",
      "\n",
      "Epoch:    5 | Iter:  211 | Fidelity: 0.932791 | Loss: 0.032219\n",
      "\n",
      "Epoch:    5 | Iter:  221 | Fidelity: 0.942077 | Loss: 0.027766\n",
      "\n",
      "Epoch:    5 | Iter:  231 | Fidelity: 0.950824 | Loss: 0.023569\n",
      "\n",
      "Epoch:    5 | Iter:  241 | Fidelity: 0.958894 | Loss: 0.019693\n",
      "\n",
      "Epoch:    5 | Iter:  251 | Fidelity: 0.966181 | Loss: 0.016205\n",
      "\n",
      "Epoch:    5 | Iter:  261 | Fidelity: 0.972616 | Loss: 0.013117\n",
      "\n",
      "Epoch:    5 | Iter:  271 | Fidelity: 0.978170 | Loss: 0.010456\n",
      "\n",
      "Epoch:    5 | Iter:  281 | Fidelity: 0.982858 | Loss: 0.008208\n",
      "\n",
      "Epoch:    5 | Iter:  291 | Fidelity: 0.986729 | Loss: 0.006356\n",
      "\n",
      "Epoch:    6 | Iter:    1 | Fidelity: 0.989859 | Loss: 0.004858\n",
      "\n",
      "Epoch:    6 | Iter:   11 | Fidelity: 0.992341 | Loss: 0.003666\n",
      "\n",
      "Epoch:    6 | Iter:   21 | Fidelity: 0.994275 | Loss: 0.002742\n",
      "\n",
      "Epoch:    6 | Iter:   31 | Fidelity: 0.995759 | Loss: 0.002031\n",
      "\n",
      "Epoch:    6 | Iter:   41 | Fidelity: 0.996882 | Loss: 0.001494\n",
      "\n",
      "Epoch:    6 | Iter:   51 | Fidelity: 0.997721 | Loss: 0.001090\n",
      "\n",
      "Epoch:    6 | Iter:   61 | Fidelity: 0.998343 | Loss: 0.000794\n",
      "\n",
      "Epoch:    6 | Iter:   71 | Fidelity: 0.998800 | Loss: 0.000572\n",
      "\n",
      "Epoch:    6 | Iter:   81 | Fidelity: 0.999133 | Loss: 0.000412\n",
      "\n",
      "Epoch:    6 | Iter:   91 | Fidelity: 0.999375 | Loss: 0.000299\n",
      "\n",
      "Epoch:    6 | Iter:  101 | Fidelity: 0.999551 | Loss: 0.000215\n",
      "\n",
      "Epoch:    6 | Iter:  111 | Fidelity: 0.999677 | Loss: 0.000151\n",
      "\n",
      "Epoch:    6 | Iter:  121 | Fidelity: 0.999768 | Loss: 0.000110\n",
      "\n",
      "Epoch:    6 | Iter:  131 | Fidelity: 0.999833 | Loss: 0.000080\n",
      "\n",
      "Epoch:    6 | Iter:  141 | Fidelity: 0.999881 | Loss: 0.000058\n",
      "\n",
      "Epoch:    6 | Iter:  151 | Fidelity: 0.999914 | Loss: 0.000043\n",
      "\n",
      "Epoch:    6 | Iter:  161 | Fidelity: 0.999938 | Loss: 0.000031\n",
      "\n",
      "Epoch:    6 | Iter:  171 | Fidelity: 0.999956 | Loss: 0.000021\n",
      "\n",
      "Epoch:    6 | Iter:  181 | Fidelity: 0.999969 | Loss: 0.000017\n",
      "\n",
      "Epoch:    6 | Iter:  191 | Fidelity: 0.999977 | Loss: 0.000010\n",
      "\n",
      "Epoch:    6 | Iter:  201 | Fidelity: 0.999984 | Loss: 0.000010\n",
      "\n",
      "Epoch:    6 | Iter:  211 | Fidelity: 0.999988 | Loss: 0.000004\n",
      "\n",
      "Epoch:    6 | Iter:  221 | Fidelity: 0.999991 | Loss: 0.000007\n",
      "\n",
      "Epoch:    6 | Iter:  231 | Fidelity: 0.999994 | Loss: 0.000002\n",
      "\n",
      "Epoch:    6 | Iter:  241 | Fidelity: 0.999996 | Loss: -0.000001\n",
      "\n",
      "Epoch:    6 | Iter:  251 | Fidelity: 0.999997 | Loss: 0.000003\n",
      "\n",
      "Epoch:    6 | Iter:  261 | Fidelity: 0.999998 | Loss: -0.000003\n",
      "\n",
      "Epoch:    6 | Iter:  271 | Fidelity: 0.999998 | Loss: 0.000001\n",
      "\n",
      "Epoch:    6 | Iter:  281 | Fidelity: 0.999998 | Loss: 0.000004\n",
      "\n",
      "Epoch:    6 | Iter:  291 | Fidelity: 0.999999 | Loss: -0.000001\n",
      "\n",
      "Fidelity threshold of 0.99 reached. Stopping training.\n",
      "\n",
      "Training finished after 6 epochs.\n",
      "\n",
      "Run took: 306.09070930001326 time.\n",
      "Generator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-gen(hs).pkl\n",
      "Discriminator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-dis(hs).pkl\n"
     ]
    }
   ],
   "source": [
    "res2 = training_instance2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f39019",
   "metadata": {},
   "source": [
    "Let us print the new parameters after training to make sure they are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "712952f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha \n",
      " Parameter containing:\n",
      "tensor([[-2.6642e-02, -2.3615e-11,  4.8779e-08, -1.0742e+00],\n",
      "        [ 7.4062e-10, -1.3119e+00,  1.2833e+00, -1.9250e-05],\n",
      "        [-4.2796e-11, -9.8995e-01,  3.8705e-01,  1.7645e-08],\n",
      "        [-2.6071e-02,  3.1418e-10, -1.3744e-09,  1.0512e+00],\n",
      "        [ 7.1309e-10,  1.1107e+00,  1.0865e+00, -1.5523e-05],\n",
      "        [-5.2268e-11, -1.1275e+00, -4.4084e-01,  3.5480e-08],\n",
      "        [ 1.1899e+00,  2.9426e-09, -1.4833e-07,  1.2250e+00]],\n",
      "       requires_grad=True)\n",
      "beta \n",
      " Parameter containing:\n",
      "tensor([[-0.4284, -0.0964, -0.1509, -0.0611],\n",
      "        [ 0.2507, -0.5719, -0.7126, -0.0708],\n",
      "        [ 0.0308,  0.5088, -0.2778,  0.1051],\n",
      "        [-0.4649,  0.0162, -0.2930, -0.4286],\n",
      "        [-0.0487,  0.1855,  0.2283, -0.4235],\n",
      "        [-0.0032, -0.5525, -0.3168,  0.0995],\n",
      "        [-0.0155, -0.0032, -0.0039, -0.0123]], requires_grad=True)\n",
      "theta \n",
      " Parameter containing:\n",
      "tensor([-2.5019e-06,  1.7390e+00, -1.5709e+00, -1.5700e+00, -2.4719e-06,\n",
      "        -4.8030e-01,  7.7924e-06,  1.2058e+00, -1.5700e+00,  1.5700e+00,\n",
      "        -2.1890e-03, -8.3096e-01,  9.6638e-06,  1.2412e+00,  2.0000e+00,\n",
      "         1.5716e+00,  5.9840e-06, -6.9936e-01, -1.3896e-05,  3.3626e-02,\n",
      "        -1.5700e+00,  1.5700e+00,  4.7566e-02,  2.1243e+00, -1.2727e-07,\n",
      "         1.0342e-01,  1.5706e+00, -5.9742e-05,  1.8232e-06, -3.3423e-01,\n",
      "        -1.4333e-05,  2.4113e-06,  6.9564e-05, -5.1527e-06, -1.0336e-01,\n",
      "         3.3430e-01], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('alpha \\n', training_instance2.dis.alpha)\n",
    "print('beta \\n', training_instance2.dis.beta)\n",
    "print('theta \\n', training_instance2.gen.ansatz.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569c4f3",
   "metadata": {},
   "source": [
    "## 4. Generator and discriminator removing ancilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67546fe",
   "metadata": {},
   "source": [
    "Now we do the opposite, we remove the unnecessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7822114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qgan.training import Training\n",
    "from config import CFG\n",
    "from tools.data.data_managers import save_model\n",
    "import os \n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2110b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = copy.deepcopy(CFG)\n",
    "config.epochs = 2\n",
    "config.extra_ancilla = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4a8e9",
   "metadata": {},
   "source": [
    "- Run training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ada8bf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== \n",
      "run_timestamp: 2025-11-13__10-04-48,\n",
      "----------------------------------------------\n",
      "load_timestamp: None,\n",
      "type_of_warm_start: none,\n",
      "warm_start_strength: 0.1,\n",
      "----------------------------------------------\n",
      "system_size: 3,\n",
      "extra_ancilla: True,\n",
      "ancilla_mode: pass,\n",
      "ancilla_project_norm: re-norm,\n",
      "ancilla_topology: bridge,\n",
      "ancilla_connect_to: None,\n",
      "do_ancilla_1q_gates: True,\n",
      "start_ancilla_gates_randomly: True,\n",
      "----------------------------------------------\n",
      "gen_layers: 3,\n",
      "gen_ansatz: ZZ_X_Z,\n",
      "----------------------------------------------\n",
      "target_hamiltonian: custom_h,\n",
      "custom_hamiltonian_terms: ['ZZZ'],\n",
      "custom_hamiltonian_strengths: [1.0],\n",
      "----------------------------------------------\n",
      "epochs: 2,\n",
      "iterations_epoch: 300,\n",
      "log_every_x_iter: 10,\n",
      "save_fid_and_loss_every_x_iter: 1,\n",
      "max_fidelity: 0.99,\n",
      "steps_gen: 1,\n",
      "steps_dis: 1,\n",
      "----------------------------------------------\n",
      "l_rate: 0.01,\n",
      "momentum_coeff: 0.9,\n",
      "================================================== \n",
      "\n",
      "\n",
      "Epoch:    1 | Iter:    1 | Fidelity: 0.022066 | Loss: -3.833749\n",
      "\n",
      "Epoch:    1 | Iter:   11 | Fidelity: 0.022932 | Loss: -3.508311\n",
      "\n",
      "Epoch:    1 | Iter:   21 | Fidelity: 0.024110 | Loss: -3.427010\n",
      "\n",
      "Epoch:    1 | Iter:   31 | Fidelity: 0.025569 | Loss: -3.360176\n",
      "\n",
      "Epoch:    1 | Iter:   41 | Fidelity: 0.027505 | Loss: -3.292915\n",
      "\n",
      "Epoch:    1 | Iter:   51 | Fidelity: 0.029797 | Loss: -3.210118\n",
      "\n",
      "Epoch:    1 | Iter:   61 | Fidelity: 0.031790 | Loss: -3.080222\n",
      "\n",
      "Epoch:    1 | Iter:   71 | Fidelity: 0.030592 | Loss: -2.962190\n",
      "\n",
      "Epoch:    1 | Iter:   81 | Fidelity: 0.018833 | Loss: -2.913986\n",
      "\n",
      "Epoch:    1 | Iter:   91 | Fidelity: 0.001259 | Loss: -2.791047\n",
      "\n",
      "Epoch:    1 | Iter:  101 | Fidelity: 0.010868 | Loss: -1.674790\n",
      "\n",
      "Epoch:    1 | Iter:  111 | Fidelity: 0.032579 | Loss: -0.199928\n",
      "\n",
      "Epoch:    1 | Iter:  121 | Fidelity: 0.046758 | Loss: -0.229701\n",
      "\n",
      "Epoch:    1 | Iter:  131 | Fidelity: 0.057385 | Loss: -0.125601\n",
      "\n",
      "Epoch:    1 | Iter:  141 | Fidelity: 0.067397 | Loss: -0.100045\n",
      "\n",
      "Epoch:    1 | Iter:  151 | Fidelity: 0.078028 | Loss: -0.103953\n",
      "\n",
      "Epoch:    1 | Iter:  161 | Fidelity: 0.089784 | Loss: -0.102661\n",
      "\n",
      "Epoch:    1 | Iter:  171 | Fidelity: 0.102915 | Loss: -0.109032\n",
      "\n",
      "Epoch:    1 | Iter:  181 | Fidelity: 0.117503 | Loss: -0.115262\n",
      "\n",
      "Epoch:    1 | Iter:  191 | Fidelity: 0.133565 | Loss: -0.122696\n",
      "\n",
      "Epoch:    1 | Iter:  201 | Fidelity: 0.151063 | Loss: -0.130828\n",
      "\n",
      "Epoch:    1 | Iter:  211 | Fidelity: 0.169922 | Loss: -0.139559\n",
      "\n",
      "Epoch:    1 | Iter:  221 | Fidelity: 0.190050 | Loss: -0.148825\n",
      "\n",
      "Epoch:    1 | Iter:  231 | Fidelity: 0.211343 | Loss: -0.158546\n",
      "\n",
      "Epoch:    1 | Iter:  241 | Fidelity: 0.233676 | Loss: -0.168659\n",
      "\n",
      "Epoch:    1 | Iter:  251 | Fidelity: 0.256862 | Loss: -0.179110\n",
      "\n",
      "Epoch:    1 | Iter:  261 | Fidelity: 0.280593 | Loss: -0.189770\n",
      "\n",
      "Epoch:    1 | Iter:  271 | Fidelity: 0.304387 | Loss: -0.200454\n",
      "\n",
      "Epoch:    1 | Iter:  281 | Fidelity: 0.327582 | Loss: -0.210894\n",
      "\n",
      "Epoch:    1 | Iter:  291 | Fidelity: 0.349431 | Loss: -0.220779\n",
      "\n",
      "Epoch:    2 | Iter:    1 | Fidelity: 0.369266 | Loss: -0.229790\n",
      "\n",
      "Epoch:    2 | Iter:   11 | Fidelity: 0.386662 | Loss: -0.237744\n",
      "\n",
      "Epoch:    2 | Iter:   21 | Fidelity: 0.401505 | Loss: -0.244561\n",
      "\n",
      "Epoch:    2 | Iter:   31 | Fidelity: 0.413947 | Loss: -0.250300\n",
      "\n",
      "Epoch:    2 | Iter:   41 | Fidelity: 0.424295 | Loss: -0.255085\n",
      "\n",
      "Epoch:    2 | Iter:   51 | Fidelity: 0.432901 | Loss: -0.259063\n",
      "\n",
      "Epoch:    2 | Iter:   61 | Fidelity: 0.440091 | Loss: -0.262377\n",
      "\n",
      "Epoch:    2 | Iter:   71 | Fidelity: 0.446141 | Loss: -0.265162\n",
      "\n",
      "Epoch:    2 | Iter:   81 | Fidelity: 0.451270 | Loss: -0.267514\n",
      "\n",
      "Epoch:    2 | Iter:   91 | Fidelity: 0.455651 | Loss: -0.269510\n",
      "\n",
      "Epoch:    2 | Iter:  101 | Fidelity: 0.459423 | Loss: -0.271222\n",
      "\n",
      "Epoch:    2 | Iter:  111 | Fidelity: 0.462693 | Loss: -0.272693\n",
      "\n",
      "Epoch:    2 | Iter:  121 | Fidelity: 0.465549 | Loss: -0.273964\n",
      "\n",
      "Epoch:    2 | Iter:  131 | Fidelity: 0.468065 | Loss: -0.275084\n",
      "\n",
      "Epoch:    2 | Iter:  141 | Fidelity: 0.470297 | Loss: -0.276062\n",
      "\n",
      "Epoch:    2 | Iter:  151 | Fidelity: 0.472294 | Loss: -0.276929\n",
      "\n",
      "Epoch:    2 | Iter:  161 | Fidelity: 0.474099 | Loss: -0.277703\n",
      "\n",
      "Epoch:    2 | Iter:  171 | Fidelity: 0.475744 | Loss: -0.278398\n",
      "\n",
      "Epoch:    2 | Iter:  181 | Fidelity: 0.477259 | Loss: -0.279029\n",
      "\n",
      "Epoch:    2 | Iter:  191 | Fidelity: 0.478668 | Loss: -0.279610\n",
      "\n",
      "Epoch:    2 | Iter:  201 | Fidelity: 0.479992 | Loss: -0.280141\n",
      "\n",
      "Epoch:    2 | Iter:  211 | Fidelity: 0.481249 | Loss: -0.280643\n",
      "\n",
      "Epoch:    2 | Iter:  221 | Fidelity: 0.482455 | Loss: -0.281108\n",
      "\n",
      "Epoch:    2 | Iter:  231 | Fidelity: 0.483623 | Loss: -0.281557\n",
      "\n",
      "Epoch:    2 | Iter:  241 | Fidelity: 0.484766 | Loss: -0.281993\n",
      "\n",
      "Epoch:    2 | Iter:  251 | Fidelity: 0.485893 | Loss: -0.282407\n",
      "\n",
      "Epoch:    2 | Iter:  261 | Fidelity: 0.487015 | Loss: -0.282821\n",
      "\n",
      "Epoch:    2 | Iter:  271 | Fidelity: 0.488141 | Loss: -0.283227\n",
      "\n",
      "Epoch:    2 | Iter:  281 | Fidelity: 0.489278 | Loss: -0.283627\n",
      "\n",
      "Epoch:    2 | Iter:  291 | Fidelity: 0.490435 | Loss: -0.284040\n",
      "\n",
      "Training finished after 2 epochs.\n",
      "\n",
      "Run took: 110.41578599996865 time.\n",
      "Generator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-gen(hs).pkl\n",
      "Discriminator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-dis(hs).pkl\n"
     ]
    }
   ],
   "source": [
    "training_instance = Training(config=config)\n",
    "res = training_instance.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a104f",
   "metadata": {},
   "source": [
    "- Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d1e487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator parameters saved to ./models/training_instance_dis_ancilla2.pkl\n",
      "Generator parameters saved to ./models/training_instance_gen_ancilla2.pkl\n",
      "alpha \n",
      " Parameter containing:\n",
      "tensor([[-3.4728e-05, -2.9476e-01, -1.3123e+00, -6.7952e-04],\n",
      "        [ 2.1210e-02,  1.0855e-04,  1.3860e-04, -1.3234e+00],\n",
      "        [ 1.6576e+00, -4.9623e-06,  4.0474e-05,  9.9190e-03],\n",
      "        [-2.9678e-04, -1.7373e+00, -3.8992e-01, -1.1957e-03],\n",
      "        [ 8.8134e-01, -9.2664e-05,  1.3240e-04,  1.3474e-02],\n",
      "        [ 7.6680e-03, -1.0857e-04, -9.2765e-05, -1.1936e+00],\n",
      "        [ 8.5431e-01,  1.9082e-03,  1.4915e-03,  9.1005e-01]],\n",
      "       requires_grad=True)\n",
      "beta \n",
      " Parameter containing:\n",
      "tensor([[ 0.3925, -0.3630, -0.0967,  0.3653],\n",
      "        [-0.6703, -0.8332,  0.6570,  0.3861],\n",
      "        [ 0.0567, -0.3839,  0.3272, -0.2195],\n",
      "        [-0.6818,  0.4113, -0.3916, -0.5851],\n",
      "        [ 0.1106,  0.4089,  0.2703, -0.0821],\n",
      "        [ 0.0345,  0.4345, -0.6264, -0.0109],\n",
      "        [ 0.1041,  0.4818,  0.4074,  0.5761]], requires_grad=True)\n",
      "theta \n",
      " Parameter containing:\n",
      "tensor([-1.2568e-02,  4.2811e-02, -1.4119e-02, -1.8455e+00, -4.1742e-02,\n",
      "        -9.0656e-01, -1.2663e+00, -4.0203e-01, -1.2374e+00, -6.4124e-01,\n",
      "        -4.2381e-01, -4.6116e-01,  4.2944e-01, -5.3935e-01,  1.4280e+00,\n",
      "         8.3969e-04,  5.8803e-02, -1.8032e-01, -4.1656e+00, -4.2593e-01,\n",
      "         1.0620e-02,  6.4530e-03, -4.8100e-01, -4.5691e-01, -4.3688e-01,\n",
      "        -1.0532e-01, -1.4287e+00,  2.6948e-01, -2.1577e-02, -1.8230e+00,\n",
      "        -1.1099e+00, -5.3506e-01, -3.4418e-01, -9.3395e-01,  1.3426e+00,\n",
      "        -9.7707e-01], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "path_dis = \"./models/training_instance_dis_ancilla2.pkl\"\n",
    "path_gen = \"./models/training_instance_gen_ancilla2.pkl\"\n",
    "training_instance.dis.save_model_params(path_dis)\n",
    "training_instance.gen.save_model_params(path_gen)\n",
    "print('alpha \\n', training_instance.dis.alpha)\n",
    "print('beta \\n', training_instance.dis.beta)\n",
    "print('theta \\n', training_instance.gen.ansatz.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097d303",
   "metadata": {},
   "source": [
    "- Load parameters in new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d38017c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = copy.deepcopy(CFG)\n",
    "config2.extra_ancilla = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "330aa328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming alpha: 7 â†’ 6\n",
      "Trimming beta: 7 â†’ 6\n",
      "Discriminator parameters loaded successfully from ./models/training_instance_dis_ancilla2.pkl\n",
      "Trimming Î¸: 36 â†’ 24\n",
      "alpha \n",
      " Parameter containing:\n",
      "tensor([[-3.4728e-05, -2.9476e-01, -1.3123e+00, -6.7952e-04],\n",
      "        [ 2.1210e-02,  1.0855e-04,  1.3860e-04, -1.3234e+00],\n",
      "        [ 1.6576e+00, -4.9623e-06,  4.0474e-05,  9.9190e-03],\n",
      "        [-2.9678e-04, -1.7373e+00, -3.8992e-01, -1.1957e-03],\n",
      "        [ 8.8134e-01, -9.2664e-05,  1.3240e-04,  1.3474e-02],\n",
      "        [ 7.6680e-03, -1.0857e-04, -9.2765e-05, -1.1936e+00]],\n",
      "       requires_grad=True)\n",
      "beta \n",
      " Parameter containing:\n",
      "tensor([[ 0.3925, -0.3630, -0.0967,  0.3653],\n",
      "        [-0.6703, -0.8332,  0.6570,  0.3861],\n",
      "        [ 0.0567, -0.3839,  0.3272, -0.2195],\n",
      "        [-0.6818,  0.4113, -0.3916, -0.5851],\n",
      "        [ 0.1106,  0.4089,  0.2703, -0.0821],\n",
      "        [ 0.0345,  0.4345, -0.6264, -0.0109]], requires_grad=True)\n",
      "theta \n",
      " Parameter containing:\n",
      "tensor([-1.2568e-02,  4.2811e-02, -1.4119e-02, -1.8455e+00, -4.1742e-02,\n",
      "        -9.0656e-01, -1.2663e+00, -4.0203e-01, -1.2374e+00, -6.4124e-01,\n",
      "        -4.2381e-01, -4.6116e-01,  4.2944e-01, -5.3935e-01,  1.4280e+00,\n",
      "         8.3969e-04,  5.8803e-02, -1.8032e-01, -4.1656e+00, -4.2593e-01,\n",
      "         1.0620e-02,  6.4530e-03, -4.8100e-01, -4.5691e-01],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "path = \"./models/training_instance_dis.pkl\"\n",
    "training_instance2 = Training(config=config2)\n",
    "training_instance2.dis.load_model_params(path_dis)\n",
    "training_instance2.gen.load_model_params(path_gen)\n",
    "print('alpha \\n', training_instance2.dis.alpha)\n",
    "print('beta \\n', training_instance2.dis.beta)\n",
    "print('theta \\n', training_instance2.gen.ansatz.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de47a8",
   "metadata": {},
   "source": [
    "- Retrain training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e94ee74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== \n",
      "run_timestamp: 2025-11-13__10-04-48,\n",
      "----------------------------------------------\n",
      "load_timestamp: None,\n",
      "type_of_warm_start: none,\n",
      "warm_start_strength: 0.1,\n",
      "----------------------------------------------\n",
      "system_size: 3,\n",
      "extra_ancilla: False,\n",
      "ancilla_mode: pass,\n",
      "ancilla_project_norm: re-norm,\n",
      "ancilla_topology: bridge,\n",
      "ancilla_connect_to: None,\n",
      "do_ancilla_1q_gates: True,\n",
      "start_ancilla_gates_randomly: True,\n",
      "----------------------------------------------\n",
      "gen_layers: 3,\n",
      "gen_ansatz: ZZ_X_Z,\n",
      "----------------------------------------------\n",
      "target_hamiltonian: custom_h,\n",
      "custom_hamiltonian_terms: ['ZZZ'],\n",
      "custom_hamiltonian_strengths: [1.0],\n",
      "----------------------------------------------\n",
      "epochs: 10,\n",
      "iterations_epoch: 300,\n",
      "log_every_x_iter: 10,\n",
      "save_fid_and_loss_every_x_iter: 1,\n",
      "max_fidelity: 0.99,\n",
      "steps_gen: 1,\n",
      "steps_dis: 1,\n",
      "----------------------------------------------\n",
      "l_rate: 0.01,\n",
      "momentum_coeff: 0.9,\n",
      "================================================== \n",
      "\n",
      "\n",
      "Epoch:    1 | Iter:    1 | Fidelity: 0.038530 | Loss: -0.736581\n",
      "\n",
      "Epoch:    1 | Iter:   11 | Fidelity: 0.041098 | Loss: -0.246502\n",
      "\n",
      "Epoch:    1 | Iter:   21 | Fidelity: 0.046152 | Loss: -0.074001\n",
      "\n",
      "Epoch:    1 | Iter:   31 | Fidelity: 0.052922 | Loss: -0.103940\n",
      "\n",
      "Epoch:    1 | Iter:   41 | Fidelity: 0.061249 | Loss: -0.087884\n",
      "\n",
      "Epoch:    1 | Iter:   51 | Fidelity: 0.071212 | Loss: -0.084961\n",
      "\n",
      "Epoch:    1 | Iter:   61 | Fidelity: 0.083005 | Loss: -0.091328\n",
      "\n",
      "Epoch:    1 | Iter:   71 | Fidelity: 0.096862 | Loss: -0.097505\n",
      "\n",
      "Epoch:    1 | Iter:   81 | Fidelity: 0.113016 | Loss: -0.104743\n",
      "\n",
      "Epoch:    1 | Iter:   91 | Fidelity: 0.131635 | Loss: -0.113576\n",
      "\n",
      "Epoch:    1 | Iter:  101 | Fidelity: 0.152771 | Loss: -0.123618\n",
      "\n",
      "Epoch:    1 | Iter:  111 | Fidelity: 0.176303 | Loss: -0.134763\n",
      "\n",
      "Epoch:    1 | Iter:  121 | Fidelity: 0.201895 | Loss: -0.146878\n",
      "\n",
      "Epoch:    1 | Iter:  131 | Fidelity: 0.228983 | Loss: -0.159663\n",
      "\n",
      "Epoch:    1 | Iter:  141 | Fidelity: 0.256800 | Loss: -0.172744\n",
      "\n",
      "Epoch:    1 | Iter:  151 | Fidelity: 0.284456 | Loss: -0.185704\n",
      "\n",
      "Epoch:    1 | Iter:  161 | Fidelity: 0.311040 | Loss: -0.198109\n",
      "\n",
      "Epoch:    1 | Iter:  171 | Fidelity: 0.335746 | Loss: -0.209585\n",
      "\n",
      "Epoch:    1 | Iter:  181 | Fidelity: 0.357973 | Loss: -0.219854\n",
      "\n",
      "Epoch:    1 | Iter:  191 | Fidelity: 0.377382 | Loss: -0.228765\n",
      "\n",
      "Epoch:    1 | Iter:  201 | Fidelity: 0.393898 | Loss: -0.236299\n",
      "\n",
      "Epoch:    1 | Iter:  211 | Fidelity: 0.407671 | Loss: -0.242522\n",
      "\n",
      "Epoch:    1 | Iter:  221 | Fidelity: 0.418994 | Loss: -0.247581\n",
      "\n",
      "Epoch:    1 | Iter:  231 | Fidelity: 0.428229 | Loss: -0.251647\n",
      "\n",
      "Epoch:    1 | Iter:  241 | Fidelity: 0.435744 | Loss: -0.254899\n",
      "\n",
      "Epoch:    1 | Iter:  251 | Fidelity: 0.441875 | Loss: -0.257485\n",
      "\n",
      "Epoch:    1 | Iter:  261 | Fidelity: 0.446903 | Loss: -0.259550\n",
      "\n",
      "Epoch:    1 | Iter:  271 | Fidelity: 0.451056 | Loss: -0.261182\n",
      "\n",
      "Epoch:    1 | Iter:  281 | Fidelity: 0.454511 | Loss: -0.262481\n",
      "\n",
      "Epoch:    1 | Iter:  291 | Fidelity: 0.457405 | Loss: -0.263510\n",
      "\n",
      "Epoch:    2 | Iter:    1 | Fidelity: 0.459843 | Loss: -0.264300\n",
      "\n",
      "Epoch:    2 | Iter:   11 | Fidelity: 0.461908 | Loss: -0.264901\n",
      "\n",
      "Epoch:    2 | Iter:   21 | Fidelity: 0.463663 | Loss: -0.265343\n",
      "\n",
      "Epoch:    2 | Iter:   31 | Fidelity: 0.465159 | Loss: -0.265644\n",
      "\n",
      "Epoch:    2 | Iter:   41 | Fidelity: 0.466436 | Loss: -0.265816\n",
      "\n",
      "Epoch:    2 | Iter:   51 | Fidelity: 0.467529 | Loss: -0.265884\n",
      "\n",
      "Epoch:    2 | Iter:   61 | Fidelity: 0.468465 | Loss: -0.265849\n",
      "\n",
      "Epoch:    2 | Iter:   71 | Fidelity: 0.469267 | Loss: -0.265736\n",
      "\n",
      "Epoch:    2 | Iter:   81 | Fidelity: 0.469953 | Loss: -0.265537\n",
      "\n",
      "Epoch:    2 | Iter:   91 | Fidelity: 0.470540 | Loss: -0.265264\n",
      "\n",
      "Epoch:    2 | Iter:  101 | Fidelity: 0.471041 | Loss: -0.264920\n",
      "\n",
      "Epoch:    2 | Iter:  111 | Fidelity: 0.471467 | Loss: -0.264505\n",
      "\n",
      "Epoch:    2 | Iter:  121 | Fidelity: 0.471828 | Loss: -0.264033\n",
      "\n",
      "Epoch:    2 | Iter:  131 | Fidelity: 0.472131 | Loss: -0.263485\n",
      "\n",
      "Epoch:    2 | Iter:  141 | Fidelity: 0.472381 | Loss: -0.262874\n",
      "\n",
      "Epoch:    2 | Iter:  151 | Fidelity: 0.472585 | Loss: -0.262194\n",
      "\n",
      "Epoch:    2 | Iter:  161 | Fidelity: 0.472745 | Loss: -0.261426\n",
      "\n",
      "Epoch:    2 | Iter:  171 | Fidelity: 0.472865 | Loss: -0.260576\n",
      "\n",
      "Epoch:    2 | Iter:  181 | Fidelity: 0.472945 | Loss: -0.259629\n",
      "\n",
      "Epoch:    2 | Iter:  191 | Fidelity: 0.472987 | Loss: -0.258572\n",
      "\n",
      "Epoch:    2 | Iter:  201 | Fidelity: 0.472991 | Loss: -0.257371\n",
      "\n",
      "Epoch:    2 | Iter:  211 | Fidelity: 0.472955 | Loss: -0.256015\n",
      "\n",
      "Epoch:    2 | Iter:  221 | Fidelity: 0.472876 | Loss: -0.254451\n",
      "\n",
      "Epoch:    2 | Iter:  231 | Fidelity: 0.472749 | Loss: -0.252625\n",
      "\n",
      "Epoch:    2 | Iter:  241 | Fidelity: 0.472567 | Loss: -0.250473\n",
      "\n",
      "Epoch:    2 | Iter:  251 | Fidelity: 0.472318 | Loss: -0.247870\n",
      "\n",
      "Epoch:    2 | Iter:  261 | Fidelity: 0.471984 | Loss: -0.244676\n",
      "\n",
      "Epoch:    2 | Iter:  271 | Fidelity: 0.471538 | Loss: -0.240630\n",
      "\n",
      "Epoch:    2 | Iter:  281 | Fidelity: 0.470933 | Loss: -0.235374\n",
      "\n",
      "Epoch:    2 | Iter:  291 | Fidelity: 0.470093 | Loss: -0.228293\n",
      "\n",
      "Epoch:    3 | Iter:    1 | Fidelity: 0.468873 | Loss: -0.218364\n",
      "\n",
      "Epoch:    3 | Iter:   11 | Fidelity: 0.466997 | Loss: -0.203846\n",
      "\n",
      "Epoch:    3 | Iter:   21 | Fidelity: 0.463842 | Loss: -0.181902\n",
      "\n",
      "Epoch:    3 | Iter:   31 | Fidelity: 0.457783 | Loss: -0.149710\n",
      "\n",
      "Epoch:    3 | Iter:   41 | Fidelity: 0.443333 | Loss: -0.121130\n",
      "\n",
      "Epoch:    3 | Iter:   51 | Fidelity: 0.395074 | Loss: -0.242729\n",
      "\n",
      "Epoch:    3 | Iter:   61 | Fidelity: 0.251047 | Loss: -0.351771\n",
      "\n",
      "Epoch:    3 | Iter:   71 | Fidelity: 0.161825 | Loss: -0.160107\n",
      "\n",
      "Epoch:    3 | Iter:   81 | Fidelity: 0.148537 | Loss: -0.134472\n",
      "\n",
      "Epoch:    3 | Iter:   91 | Fidelity: 0.165144 | Loss: -0.135489\n",
      "\n",
      "Epoch:    3 | Iter:  101 | Fidelity: 0.194360 | Loss: -0.145926\n",
      "\n",
      "Epoch:    3 | Iter:  111 | Fidelity: 0.228872 | Loss: -0.159718\n",
      "\n",
      "Epoch:    3 | Iter:  121 | Fidelity: 0.264322 | Loss: -0.174147\n",
      "\n",
      "Epoch:    3 | Iter:  131 | Fidelity: 0.297536 | Loss: -0.187840\n",
      "\n",
      "Epoch:    3 | Iter:  141 | Fidelity: 0.326356 | Loss: -0.199835\n",
      "\n",
      "Epoch:    3 | Iter:  151 | Fidelity: 0.349822 | Loss: -0.209702\n",
      "\n",
      "Epoch:    3 | Iter:  161 | Fidelity: 0.368117 | Loss: -0.217509\n",
      "\n",
      "Epoch:    3 | Iter:  171 | Fidelity: 0.382211 | Loss: -0.223652\n",
      "\n",
      "Epoch:    3 | Iter:  181 | Fidelity: 0.393378 | Loss: -0.228641\n",
      "\n",
      "Epoch:    3 | Iter:  191 | Fidelity: 0.402823 | Loss: -0.232932\n",
      "\n",
      "Epoch:    3 | Iter:  201 | Fidelity: 0.411477 | Loss: -0.236876\n",
      "\n",
      "Epoch:    3 | Iter:  211 | Fidelity: 0.419920 | Loss: -0.240642\n",
      "\n",
      "Epoch:    3 | Iter:  221 | Fidelity: 0.428376 | Loss: -0.244237\n",
      "\n",
      "Epoch:    3 | Iter:  231 | Fidelity: 0.436747 | Loss: -0.247523\n",
      "\n",
      "Epoch:    3 | Iter:  241 | Fidelity: 0.444708 | Loss: -0.250263\n",
      "\n",
      "Epoch:    3 | Iter:  251 | Fidelity: 0.451846 | Loss: -0.252181\n",
      "\n",
      "Epoch:    3 | Iter:  261 | Fidelity: 0.457810 | Loss: -0.253027\n",
      "\n",
      "Epoch:    3 | Iter:  271 | Fidelity: 0.462404 | Loss: -0.252605\n",
      "\n",
      "Epoch:    3 | Iter:  281 | Fidelity: 0.465596 | Loss: -0.250773\n",
      "\n",
      "Epoch:    3 | Iter:  291 | Fidelity: 0.467445 | Loss: -0.247377\n",
      "\n",
      "Epoch:    4 | Iter:    1 | Fidelity: 0.467981 | Loss: -0.242219\n",
      "\n",
      "Epoch:    4 | Iter:   11 | Fidelity: 0.467073 | Loss: -0.235051\n",
      "\n",
      "Epoch:    4 | Iter:   21 | Fidelity: 0.464248 | Loss: -0.225706\n",
      "\n",
      "Epoch:    4 | Iter:   31 | Fidelity: 0.458392 | Loss: -0.214606\n",
      "\n",
      "Epoch:    4 | Iter:   41 | Fidelity: 0.447243 | Loss: -0.204346\n",
      "\n",
      "Epoch:    4 | Iter:   51 | Fidelity: 0.426803 | Loss: -0.203041\n",
      "\n",
      "Epoch:    4 | Iter:   61 | Fidelity: 0.392419 | Loss: -0.222019\n",
      "\n",
      "Epoch:    4 | Iter:   71 | Fidelity: 0.346391 | Loss: -0.239233\n",
      "\n",
      "Epoch:    4 | Iter:   81 | Fidelity: 0.306761 | Loss: -0.220937\n",
      "\n",
      "Epoch:    4 | Iter:   91 | Fidelity: 0.289732 | Loss: -0.201532\n",
      "\n",
      "Epoch:    4 | Iter:  101 | Fidelity: 0.293755 | Loss: -0.195577\n",
      "\n",
      "Epoch:    4 | Iter:  111 | Fidelity: 0.310119 | Loss: -0.197921\n",
      "\n",
      "Epoch:    4 | Iter:  121 | Fidelity: 0.331236 | Loss: -0.203709\n",
      "\n",
      "Epoch:    4 | Iter:  131 | Fidelity: 0.352179 | Loss: -0.210132\n",
      "\n",
      "Epoch:    4 | Iter:  141 | Fidelity: 0.370396 | Loss: -0.215795\n",
      "\n",
      "Epoch:    4 | Iter:  151 | Fidelity: 0.385111 | Loss: -0.220191\n",
      "\n",
      "Epoch:    4 | Iter:  161 | Fidelity: 0.396656 | Loss: -0.223316\n",
      "\n",
      "Epoch:    4 | Iter:  171 | Fidelity: 0.405831 | Loss: -0.225359\n",
      "\n",
      "Epoch:    4 | Iter:  181 | Fidelity: 0.413443 | Loss: -0.226550\n",
      "\n",
      "Epoch:    4 | Iter:  191 | Fidelity: 0.420035 | Loss: -0.226975\n",
      "\n",
      "Epoch:    4 | Iter:  201 | Fidelity: 0.425773 | Loss: -0.226659\n",
      "\n",
      "Epoch:    4 | Iter:  211 | Fidelity: 0.430406 | Loss: -0.225551\n",
      "\n",
      "Epoch:    4 | Iter:  221 | Fidelity: 0.433255 | Loss: -0.223746\n",
      "\n",
      "Epoch:    4 | Iter:  231 | Fidelity: 0.433200 | Loss: -0.221689\n",
      "\n",
      "Epoch:    4 | Iter:  241 | Fidelity: 0.428719 | Loss: -0.220428\n",
      "\n",
      "Epoch:    4 | Iter:  251 | Fidelity: 0.418242 | Loss: -0.221278\n",
      "\n",
      "Epoch:    4 | Iter:  261 | Fidelity: 0.401384 | Loss: -0.224024\n",
      "\n",
      "Epoch:    4 | Iter:  271 | Fidelity: 0.380867 | Loss: -0.225374\n",
      "\n",
      "Epoch:    4 | Iter:  281 | Fidelity: 0.362616 | Loss: -0.222618\n",
      "\n",
      "Epoch:    4 | Iter:  291 | Fidelity: 0.352121 | Loss: -0.217988\n",
      "\n",
      "Epoch:    5 | Iter:    1 | Fidelity: 0.351057 | Loss: -0.215031\n",
      "\n",
      "Epoch:    5 | Iter:   11 | Fidelity: 0.357652 | Loss: -0.214789\n",
      "\n",
      "Epoch:    5 | Iter:   21 | Fidelity: 0.368821 | Loss: -0.216489\n",
      "\n",
      "Epoch:    5 | Iter:   31 | Fidelity: 0.381697 | Loss: -0.218920\n",
      "\n",
      "Epoch:    5 | Iter:   41 | Fidelity: 0.394211 | Loss: -0.221095\n",
      "\n",
      "Epoch:    5 | Iter:   51 | Fidelity: 0.405094 | Loss: -0.222399\n",
      "\n",
      "Epoch:    5 | Iter:   61 | Fidelity: 0.413651 | Loss: -0.222534\n",
      "\n",
      "Epoch:    5 | Iter:   71 | Fidelity: 0.419476 | Loss: -0.221548\n",
      "\n",
      "Epoch:    5 | Iter:   81 | Fidelity: 0.422187 | Loss: -0.219836\n",
      "\n",
      "Epoch:    5 | Iter:   91 | Fidelity: 0.421223 | Loss: -0.218320\n",
      "\n",
      "Epoch:    5 | Iter:  101 | Fidelity: 0.415745 | Loss: -0.218431\n",
      "\n",
      "Epoch:    5 | Iter:  111 | Fidelity: 0.404953 | Loss: -0.221137\n",
      "\n",
      "Epoch:    5 | Iter:  121 | Fidelity: 0.389340 | Loss: -0.224794\n",
      "\n",
      "Epoch:    5 | Iter:  131 | Fidelity: 0.372464 | Loss: -0.225395\n",
      "\n",
      "Epoch:    5 | Iter:  141 | Fidelity: 0.360152 | Loss: -0.222257\n",
      "\n",
      "Epoch:    5 | Iter:  151 | Fidelity: 0.356319 | Loss: -0.218918\n",
      "\n",
      "Epoch:    5 | Iter:  161 | Fidelity: 0.360781 | Loss: -0.217801\n",
      "\n",
      "Epoch:    5 | Iter:  171 | Fidelity: 0.370819 | Loss: -0.218800\n",
      "\n",
      "Epoch:    5 | Iter:  181 | Fidelity: 0.383321 | Loss: -0.220764\n",
      "\n",
      "Epoch:    5 | Iter:  191 | Fidelity: 0.395877 | Loss: -0.222597\n",
      "\n",
      "Epoch:    5 | Iter:  201 | Fidelity: 0.406993 | Loss: -0.223585\n",
      "\n",
      "Epoch:    5 | Iter:  211 | Fidelity: 0.415908 | Loss: -0.223384\n",
      "\n",
      "Epoch:    5 | Iter:  221 | Fidelity: 0.422284 | Loss: -0.222070\n",
      "\n",
      "Epoch:    5 | Iter:  231 | Fidelity: 0.425916 | Loss: -0.220057\n",
      "\n",
      "Epoch:    5 | Iter:  241 | Fidelity: 0.426501 | Loss: -0.218236\n",
      "\n",
      "Epoch:    5 | Iter:  251 | Fidelity: 0.423533 | Loss: -0.217749\n",
      "\n",
      "Epoch:    5 | Iter:  261 | Fidelity: 0.416477 | Loss: -0.219311\n",
      "\n",
      "Epoch:    5 | Iter:  271 | Fidelity: 0.405477 | Loss: -0.221969\n",
      "\n",
      "Epoch:    5 | Iter:  281 | Fidelity: 0.392392 | Loss: -0.223258\n",
      "\n",
      "Epoch:    5 | Iter:  291 | Fidelity: 0.380800 | Loss: -0.221933\n",
      "\n",
      "Epoch:    6 | Iter:    1 | Fidelity: 0.374111 | Loss: -0.219422\n",
      "\n",
      "Epoch:    6 | Iter:   11 | Fidelity: 0.373634 | Loss: -0.217684\n",
      "\n",
      "Epoch:    6 | Iter:   21 | Fidelity: 0.378498 | Loss: -0.217400\n",
      "\n",
      "Epoch:    6 | Iter:   31 | Fidelity: 0.386779 | Loss: -0.218207\n",
      "\n",
      "Epoch:    6 | Iter:   41 | Fidelity: 0.396515 | Loss: -0.219415\n",
      "\n",
      "Epoch:    6 | Iter:   51 | Fidelity: 0.406208 | Loss: -0.220448\n",
      "\n",
      "Epoch:    6 | Iter:   61 | Fidelity: 0.414908 | Loss: -0.220972\n",
      "\n",
      "Epoch:    6 | Iter:   71 | Fidelity: 0.422113 | Loss: -0.220897\n",
      "\n",
      "Epoch:    6 | Iter:   81 | Fidelity: 0.427602 | Loss: -0.220323\n",
      "\n",
      "Epoch:    6 | Iter:   91 | Fidelity: 0.431286 | Loss: -0.219450\n",
      "\n",
      "Epoch:    6 | Iter:  101 | Fidelity: 0.433102 | Loss: -0.218482\n",
      "\n",
      "Epoch:    6 | Iter:  111 | Fidelity: 0.432965 | Loss: -0.217553\n",
      "\n",
      "Epoch:    6 | Iter:  121 | Fidelity: 0.430773 | Loss: -0.216739\n",
      "\n",
      "Epoch:    6 | Iter:  131 | Fidelity: 0.426483 | Loss: -0.216110\n",
      "\n",
      "Epoch:    6 | Iter:  141 | Fidelity: 0.420197 | Loss: -0.215835\n",
      "\n",
      "Epoch:    6 | Iter:  151 | Fidelity: 0.412212 | Loss: -0.216087\n",
      "\n",
      "Epoch:    6 | Iter:  161 | Fidelity: 0.403008 | Loss: -0.216936\n",
      "\n",
      "Epoch:    6 | Iter:  171 | Fidelity: 0.393241 | Loss: -0.218001\n",
      "\n",
      "Epoch:    6 | Iter:  181 | Fidelity: 0.383888 | Loss: -0.218579\n",
      "\n",
      "Epoch:    6 | Iter:  191 | Fidelity: 0.376323 | Loss: -0.218298\n",
      "\n",
      "Epoch:    6 | Iter:  201 | Fidelity: 0.371945 | Loss: -0.217550\n",
      "\n",
      "Epoch:    6 | Iter:  211 | Fidelity: 0.371480 | Loss: -0.217070\n",
      "\n",
      "Epoch:    6 | Iter:  221 | Fidelity: 0.374674 | Loss: -0.217290\n",
      "\n",
      "Epoch:    6 | Iter:  231 | Fidelity: 0.380569 | Loss: -0.218165\n",
      "\n",
      "Epoch:    6 | Iter:  241 | Fidelity: 0.387962 | Loss: -0.219378\n",
      "\n",
      "Epoch:    6 | Iter:  251 | Fidelity: 0.395748 | Loss: -0.220538\n",
      "\n",
      "Epoch:    6 | Iter:  261 | Fidelity: 0.403048 | Loss: -0.221293\n",
      "\n",
      "Epoch:    6 | Iter:  271 | Fidelity: 0.409225 | Loss: -0.221478\n",
      "\n",
      "Epoch:    6 | Iter:  281 | Fidelity: 0.413837 | Loss: -0.221031\n",
      "\n",
      "Epoch:    6 | Iter:  291 | Fidelity: 0.416569 | Loss: -0.220114\n",
      "\n",
      "Epoch:    7 | Iter:    1 | Fidelity: 0.417166 | Loss: -0.219053\n",
      "\n",
      "Epoch:    7 | Iter:   11 | Fidelity: 0.415403 | Loss: -0.218340\n",
      "\n",
      "Epoch:    7 | Iter:   21 | Fidelity: 0.411114 | Loss: -0.218404\n",
      "\n",
      "Epoch:    7 | Iter:   31 | Fidelity: 0.404378 | Loss: -0.219275\n",
      "\n",
      "Epoch:    7 | Iter:   41 | Fidelity: 0.395864 | Loss: -0.220253\n",
      "\n",
      "Epoch:    7 | Iter:   51 | Fidelity: 0.387120 | Loss: -0.220339\n",
      "\n",
      "Epoch:    7 | Iter:   61 | Fidelity: 0.380285 | Loss: -0.219293\n",
      "\n",
      "Epoch:    7 | Iter:   71 | Fidelity: 0.377134 | Loss: -0.217846\n",
      "\n",
      "Epoch:    7 | Iter:   81 | Fidelity: 0.378267 | Loss: -0.216875\n",
      "\n",
      "Epoch:    7 | Iter:   91 | Fidelity: 0.383097 | Loss: -0.216697\n",
      "\n",
      "Epoch:    7 | Iter:  101 | Fidelity: 0.390424 | Loss: -0.217160\n",
      "\n",
      "Epoch:    7 | Iter:  111 | Fidelity: 0.399009 | Loss: -0.217937\n",
      "\n",
      "Epoch:    7 | Iter:  121 | Fidelity: 0.407894 | Loss: -0.218763\n",
      "\n",
      "Epoch:    7 | Iter:  131 | Fidelity: 0.416492 | Loss: -0.219599\n",
      "\n",
      "Epoch:    7 | Iter:  141 | Fidelity: 0.424514 | Loss: -0.220534\n",
      "\n",
      "Epoch:    7 | Iter:  151 | Fidelity: 0.431839 | Loss: -0.221782\n",
      "\n",
      "Epoch:    7 | Iter:  161 | Fidelity: 0.438328 | Loss: -0.223434\n",
      "\n",
      "Epoch:    7 | Iter:  171 | Fidelity: 0.443619 | Loss: -0.225314\n",
      "\n",
      "Epoch:    7 | Iter:  181 | Fidelity: 0.446954 | Loss: -0.226785\n",
      "\n",
      "Epoch:    7 | Iter:  191 | Fidelity: 0.447216 | Loss: -0.226769\n",
      "\n",
      "Epoch:    7 | Iter:  201 | Fidelity: 0.443347 | Loss: -0.223970\n",
      "\n",
      "Epoch:    7 | Iter:  211 | Fidelity: 0.435008 | Loss: -0.217234\n",
      "\n",
      "Epoch:    7 | Iter:  221 | Fidelity: 0.422868 | Loss: -0.205809\n",
      "\n",
      "Epoch:    7 | Iter:  231 | Fidelity: 0.408084 | Loss: -0.189006\n",
      "\n",
      "Epoch:    7 | Iter:  241 | Fidelity: 0.391252 | Loss: -0.165446\n",
      "\n",
      "Epoch:    7 | Iter:  251 | Fidelity: 0.371364 | Loss: -0.132752\n",
      "\n",
      "Epoch:    7 | Iter:  261 | Fidelity: 0.344396 | Loss: -0.091362\n",
      "\n",
      "Epoch:    7 | Iter:  271 | Fidelity: 0.299341 | Loss: -0.071895\n",
      "\n",
      "Epoch:    7 | Iter:  281 | Fidelity: 0.211835 | Loss: -0.183878\n",
      "\n",
      "Epoch:    7 | Iter:  291 | Fidelity: 0.108123 | Loss: -0.183086\n",
      "\n",
      "Epoch:    8 | Iter:    1 | Fidelity: 0.078063 | Loss: -0.100506\n",
      "\n",
      "Epoch:    8 | Iter:   11 | Fidelity: 0.090147 | Loss: -0.095606\n",
      "\n",
      "Epoch:    8 | Iter:   21 | Fidelity: 0.124275 | Loss: -0.108518\n",
      "\n",
      "Epoch:    8 | Iter:   31 | Fidelity: 0.176859 | Loss: -0.130671\n",
      "\n",
      "Epoch:    8 | Iter:   41 | Fidelity: 0.246184 | Loss: -0.160106\n",
      "\n",
      "Epoch:    8 | Iter:   51 | Fidelity: 0.327188 | Loss: -0.194140\n",
      "\n",
      "Epoch:    8 | Iter:   61 | Fidelity: 0.410543 | Loss: -0.228564\n",
      "\n",
      "Epoch:    8 | Iter:   71 | Fidelity: 0.485719 | Loss: -0.259001\n",
      "\n",
      "Epoch:    8 | Iter:   81 | Fidelity: 0.545469 | Loss: -0.282708\n",
      "\n",
      "Epoch:    8 | Iter:   91 | Fidelity: 0.588054 | Loss: -0.299266\n",
      "\n",
      "Epoch:    8 | Iter:  101 | Fidelity: 0.616139 | Loss: -0.309959\n",
      "\n",
      "Epoch:    8 | Iter:  111 | Fidelity: 0.634063 | Loss: -0.316624\n",
      "\n",
      "Epoch:    8 | Iter:  121 | Fidelity: 0.645697 | Loss: -0.320837\n",
      "\n",
      "Epoch:    8 | Iter:  131 | Fidelity: 0.653665 | Loss: -0.323640\n",
      "\n",
      "Epoch:    8 | Iter:  141 | Fidelity: 0.659491 | Loss: -0.325632\n",
      "\n",
      "Epoch:    8 | Iter:  151 | Fidelity: 0.663998 | Loss: -0.327127\n",
      "\n",
      "Epoch:    8 | Iter:  161 | Fidelity: 0.667634 | Loss: -0.328287\n",
      "\n",
      "Epoch:    8 | Iter:  171 | Fidelity: 0.670658 | Loss: -0.329226\n",
      "\n",
      "Epoch:    8 | Iter:  181 | Fidelity: 0.673232 | Loss: -0.329994\n",
      "\n",
      "Epoch:    8 | Iter:  191 | Fidelity: 0.675466 | Loss: -0.330628\n",
      "\n",
      "Epoch:    8 | Iter:  201 | Fidelity: 0.677435 | Loss: -0.331168\n",
      "\n",
      "Epoch:    8 | Iter:  211 | Fidelity: 0.679194 | Loss: -0.331638\n",
      "\n",
      "Epoch:    8 | Iter:  221 | Fidelity: 0.680784 | Loss: -0.332055\n",
      "\n",
      "Epoch:    8 | Iter:  231 | Fidelity: 0.682231 | Loss: -0.332414\n",
      "\n",
      "Epoch:    8 | Iter:  241 | Fidelity: 0.683560 | Loss: -0.332732\n",
      "\n",
      "Epoch:    8 | Iter:  251 | Fidelity: 0.684788 | Loss: -0.333025\n",
      "\n",
      "Epoch:    8 | Iter:  261 | Fidelity: 0.685928 | Loss: -0.333288\n",
      "\n",
      "Epoch:    8 | Iter:  271 | Fidelity: 0.686990 | Loss: -0.333532\n",
      "\n",
      "Epoch:    8 | Iter:  281 | Fidelity: 0.687982 | Loss: -0.333757\n",
      "\n",
      "Epoch:    8 | Iter:  291 | Fidelity: 0.688913 | Loss: -0.333962\n",
      "\n",
      "Epoch:    9 | Iter:    1 | Fidelity: 0.689786 | Loss: -0.334149\n",
      "\n",
      "Epoch:    9 | Iter:   11 | Fidelity: 0.690609 | Loss: -0.334331\n",
      "\n",
      "Epoch:    9 | Iter:   21 | Fidelity: 0.691384 | Loss: -0.334491\n",
      "\n",
      "Epoch:    9 | Iter:   31 | Fidelity: 0.692115 | Loss: -0.334646\n",
      "\n",
      "Epoch:    9 | Iter:   41 | Fidelity: 0.692807 | Loss: -0.334794\n",
      "\n",
      "Epoch:    9 | Iter:   51 | Fidelity: 0.693460 | Loss: -0.334926\n",
      "\n",
      "Epoch:    9 | Iter:   61 | Fidelity: 0.694079 | Loss: -0.335053\n",
      "\n",
      "Epoch:    9 | Iter:   71 | Fidelity: 0.694665 | Loss: -0.335174\n",
      "\n",
      "Epoch:    9 | Iter:   81 | Fidelity: 0.695220 | Loss: -0.335281\n",
      "\n",
      "Epoch:    9 | Iter:   91 | Fidelity: 0.695748 | Loss: -0.335388\n",
      "\n",
      "Epoch:    9 | Iter:  101 | Fidelity: 0.696248 | Loss: -0.335486\n",
      "\n",
      "Epoch:    9 | Iter:  111 | Fidelity: 0.696724 | Loss: -0.335583\n",
      "\n",
      "Epoch:    9 | Iter:  121 | Fidelity: 0.697176 | Loss: -0.335673\n",
      "\n",
      "Epoch:    9 | Iter:  131 | Fidelity: 0.697606 | Loss: -0.335758\n",
      "\n",
      "Epoch:    9 | Iter:  141 | Fidelity: 0.698015 | Loss: -0.335835\n",
      "\n",
      "Epoch:    9 | Iter:  151 | Fidelity: 0.698405 | Loss: -0.335916\n",
      "\n",
      "Epoch:    9 | Iter:  161 | Fidelity: 0.698776 | Loss: -0.335984\n",
      "\n",
      "Epoch:    9 | Iter:  171 | Fidelity: 0.699130 | Loss: -0.336052\n",
      "\n",
      "Epoch:    9 | Iter:  181 | Fidelity: 0.699468 | Loss: -0.336119\n",
      "\n",
      "Epoch:    9 | Iter:  191 | Fidelity: 0.699791 | Loss: -0.336178\n",
      "\n",
      "Epoch:    9 | Iter:  201 | Fidelity: 0.700098 | Loss: -0.336236\n",
      "\n",
      "Epoch:    9 | Iter:  211 | Fidelity: 0.700392 | Loss: -0.336289\n",
      "\n",
      "Epoch:    9 | Iter:  221 | Fidelity: 0.700673 | Loss: -0.336342\n",
      "\n",
      "Epoch:    9 | Iter:  231 | Fidelity: 0.700942 | Loss: -0.336392\n",
      "\n",
      "Epoch:    9 | Iter:  241 | Fidelity: 0.701200 | Loss: -0.336445\n",
      "\n",
      "Epoch:    9 | Iter:  251 | Fidelity: 0.701446 | Loss: -0.336488\n",
      "\n",
      "Epoch:    9 | Iter:  261 | Fidelity: 0.701681 | Loss: -0.336525\n",
      "\n",
      "Epoch:    9 | Iter:  271 | Fidelity: 0.701907 | Loss: -0.336572\n",
      "\n",
      "Epoch:    9 | Iter:  281 | Fidelity: 0.702123 | Loss: -0.336609\n",
      "\n",
      "Epoch:    9 | Iter:  291 | Fidelity: 0.702330 | Loss: -0.336647\n",
      "\n",
      "Epoch:   10 | Iter:    1 | Fidelity: 0.702529 | Loss: -0.336679\n",
      "\n",
      "Epoch:   10 | Iter:   11 | Fidelity: 0.702720 | Loss: -0.336708\n",
      "\n",
      "Epoch:   10 | Iter:   21 | Fidelity: 0.702903 | Loss: -0.336745\n",
      "\n",
      "Epoch:   10 | Iter:   31 | Fidelity: 0.703079 | Loss: -0.336769\n",
      "\n",
      "Epoch:   10 | Iter:   41 | Fidelity: 0.703248 | Loss: -0.336800\n",
      "\n",
      "Epoch:   10 | Iter:   51 | Fidelity: 0.703410 | Loss: -0.336823\n",
      "\n",
      "Epoch:   10 | Iter:   61 | Fidelity: 0.703566 | Loss: -0.336848\n",
      "\n",
      "Epoch:   10 | Iter:   71 | Fidelity: 0.703716 | Loss: -0.336879\n",
      "\n",
      "Epoch:   10 | Iter:   81 | Fidelity: 0.703860 | Loss: -0.336901\n",
      "\n",
      "Epoch:   10 | Iter:   91 | Fidelity: 0.703998 | Loss: -0.336914\n",
      "\n",
      "Epoch:   10 | Iter:  101 | Fidelity: 0.704131 | Loss: -0.336931\n",
      "\n",
      "Epoch:   10 | Iter:  111 | Fidelity: 0.704259 | Loss: -0.336953\n",
      "\n",
      "Epoch:   10 | Iter:  121 | Fidelity: 0.704382 | Loss: -0.336967\n",
      "\n",
      "Epoch:   10 | Iter:  131 | Fidelity: 0.704501 | Loss: -0.336986\n",
      "\n",
      "Epoch:   10 | Iter:  141 | Fidelity: 0.704615 | Loss: -0.336998\n",
      "\n",
      "Epoch:   10 | Iter:  151 | Fidelity: 0.704726 | Loss: -0.337016\n",
      "\n",
      "Epoch:   10 | Iter:  161 | Fidelity: 0.704831 | Loss: -0.337027\n",
      "\n",
      "Epoch:   10 | Iter:  171 | Fidelity: 0.704933 | Loss: -0.337038\n",
      "\n",
      "Epoch:   10 | Iter:  181 | Fidelity: 0.705031 | Loss: -0.337049\n",
      "\n",
      "Epoch:   10 | Iter:  191 | Fidelity: 0.705126 | Loss: -0.337056\n",
      "\n",
      "Epoch:   10 | Iter:  201 | Fidelity: 0.705217 | Loss: -0.337061\n",
      "\n",
      "Epoch:   10 | Iter:  211 | Fidelity: 0.705304 | Loss: -0.337067\n",
      "\n",
      "Epoch:   10 | Iter:  221 | Fidelity: 0.705388 | Loss: -0.337070\n",
      "\n",
      "Epoch:   10 | Iter:  231 | Fidelity: 0.705470 | Loss: -0.337075\n",
      "\n",
      "Epoch:   10 | Iter:  241 | Fidelity: 0.705548 | Loss: -0.337079\n",
      "\n",
      "Epoch:   10 | Iter:  251 | Fidelity: 0.705623 | Loss: -0.337082\n",
      "\n",
      "Epoch:   10 | Iter:  261 | Fidelity: 0.705695 | Loss: -0.337080\n",
      "\n",
      "Epoch:   10 | Iter:  271 | Fidelity: 0.705765 | Loss: -0.337079\n",
      "\n",
      "Epoch:   10 | Iter:  281 | Fidelity: 0.705832 | Loss: -0.337080\n",
      "\n",
      "Epoch:   10 | Iter:  291 | Fidelity: 0.705897 | Loss: -0.337074\n",
      "\n",
      "Training finished after 10 epochs.\n",
      "\n",
      "Run took: 455.91644290008117 time.\n",
      "Generator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-gen(hs).pkl\n",
      "Discriminator parameters saved to ./generated_data/2025-11-13__10-04-48/saved_model/model-dis(hs).pkl\n"
     ]
    }
   ],
   "source": [
    "res2 = training_instance2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c633e",
   "metadata": {},
   "source": [
    "Let us print the new parameters after training to make sure they are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1d501b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha \n",
      " Parameter containing:\n",
      "tensor([[-3.1496e-08, -3.1596e-01, -1.4088e+00,  4.8695e-07],\n",
      "        [ 1.4992e-02, -5.6511e-05,  1.0509e-03, -1.4265e+00],\n",
      "        [ 1.7142e+00, -7.0453e-07, -1.8438e-06,  8.0378e-03],\n",
      "        [-9.8271e-09, -1.7827e+00, -3.9980e-01, -6.2936e-07],\n",
      "        [ 1.1415e+00, -2.4432e-05,  8.4899e-05,  1.1996e-02],\n",
      "        [ 6.7879e-03, -1.3844e-05,  8.6109e-07, -1.3268e+00]],\n",
      "       requires_grad=True)\n",
      "beta \n",
      " Parameter containing:\n",
      "tensor([[ 5.0291e-02,  1.2131e-03,  3.8592e-03,  5.0115e-01],\n",
      "        [-3.9976e-02, -2.4572e-01,  1.2281e+00,  2.1316e-02],\n",
      "        [ 1.4551e-03, -1.8216e-01,  3.3036e-01, -2.3717e-03],\n",
      "        [-8.3172e-01, -1.6236e-04, -7.1284e-04, -2.8302e-01],\n",
      "        [ 2.1191e-01,  5.1195e-01, -4.9110e-01, -2.4658e-01],\n",
      "        [-6.9180e-04,  2.3723e-01, -4.8481e-01,  9.7444e-03]],\n",
      "       requires_grad=True)\n",
      "theta \n",
      " Parameter containing:\n",
      "tensor([-1.1376e-05,  2.6151e-01,  1.5061e-01, -2.3587e+00, -1.8854e-03,\n",
      "        -4.3990e-01, -1.9512e+00, -1.7288e+00, -3.1087e-04, -2.5140e-01,\n",
      "        -1.0763e+00, -1.1454e-01,  5.3902e-02, -2.2665e-02, -7.4277e-02,\n",
      "        -7.1304e-02,  1.4044e-04, -8.6573e-03, -5.2710e+00, -7.3021e-01,\n",
      "        -5.4356e-02,  4.5551e-01, -1.1248e+00, -1.3694e+00],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('alpha \\n', training_instance2.dis.alpha)\n",
    "print('beta \\n', training_instance2.dis.beta)\n",
    "print('theta \\n', training_instance2.gen.ansatz.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8610ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
